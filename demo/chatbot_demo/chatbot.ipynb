{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å®ç°ä¸€ä¸ªåŒ…å«è®°å¿†çš„Chatbot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å›é¡¾\n",
    "ç®€å•å›é¡¾ä¸‹[tutorial_langchain.ipynb](../../tutorial_langchain.ipynb)ä¸­å…³äºchatbotçš„æœ€åŸºç¡€å®ç°ï¼Œå³ä¸€ä¸ªchat modelå’Œprompt templateçš„å®šä¹‰ï¼Œå¯ä»¥é€‰æ‹©ç”¨chainçš„æ–¹å¼è¿æ¥èµ·æ¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# è¯»å– .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡, æˆ‘ä»¬å¯ä»¥åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEYï¼Œæ–‡ä»¶å†…å®¹ç±»ä¼¼äºï¼š OPENAI_API_KEY=your_api_key\n",
    "load_dotenv() \n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv('OPENAI_API_BASE')\n",
    "\n",
    "# è°ƒç”¨langchainçš„å°è£…init_chat_modelæ–¹æ³•ï¼Œåˆå§‹åŒ–ä¸€ä¸ªchatæ¨¡å‹\n",
    "# è¿™é‡Œæˆ‘ç”¨çš„æ˜¯æ˜Ÿç«x1æ¨¡å‹ï¼Œx1å…¼å®¹openaiçš„è°ƒç”¨æ ¼å¼ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥è°ƒç”¨openaiçš„æ¥å£\n",
    "# base_urlå’Œapi_keyæ˜¯ä».env æ–‡ä»¶ä¸­è¯»å–çš„, æ¥è‡ªæ˜Ÿç«å¼€æ”¾å¹³å°\n",
    "model = init_chat_model(model = \"x1\", model_provider=\"openai\",\n",
    "                        base_url=base_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ çš„åå­—æ˜¯Retroå‘€ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ~ (â—•á´—â—•âœ¿)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"ä½ å¥½ï¼Œæˆ‘æ˜¯{name}\"\n",
    "        ),\n",
    "        AIMessagePromptTemplate.from_template(\n",
    "            \"ä½ å¥½, æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©çš„ï¼Ÿ\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆ\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parse = StrOutputParser()\n",
    "chain = prompt_template | model | parse\n",
    "chain.invoke({\"name\": \"Retro\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å†å²è®°å¿†ï¼ˆHistory Memoryï¼‰ç®¡ç†\n",
    "ref: \n",
    "- [How to add message history](https://python.langchain.com/docs/how_to/message_history/)\n",
    "- [How to add memory to chatbots](https://python.langchain.com/docs/how_to/chatbots_memory/#message-passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æ„å»ºchatbotæ—¶ï¼Œå°†ä¼šè¯çŠ¶æ€ä¼ å…¥å’Œä¼ å‡ºchainæ˜¯è‡³å…³é‡è¦çš„ã€‚LangGraphå®ç°äº†ä¸€ä¸ªå†…ç½®çš„æŒä¹…åŒ–å±‚ï¼ˆpersistence layerï¼‰ï¼Œå…è®¸chainçŠ¶æ€è‡ªåŠ¨æŒä¹…åŒ–åˆ°å†…å­˜æˆ–å¤–éƒ¨åç«¯ï¼ˆå¦‚SQLiteã€Postgresæˆ–Redisï¼‰ã€‚è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨LangGraph [persistence documentation](https://langchain-ai.github.io/langgraph/how-tos/persistence/?_gl=1*15xmk3c*_ga*MTE5OTY2ODY1MC4xNzQ3MjM2ODQ2*_ga_47WX3HKKY2*czE3NDc2NjA1NzkkbzE4JGcxJHQxNzQ3NjYzMzgxJGowJGwwJGgw)ä¸­æ‰¾åˆ°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥æˆ‘ä»¬ä¼šé€šè¿‡LangGraphçš„StateGraphç±»ï¼Œå®ç°ä¸€ä¸ªæ‹¥æœ‰è‡ªåŠ¨å†å²messagesç®¡ç†çš„chatbotï¼Œæ¥å±•ç¤ºLangGraphæ˜¯æ€ä¹ˆåšå†å²è®°å¿†ç®¡ç†çš„ã€‚\n",
    "- è¿™é‡Œæ‰€è°“çš„stateå°±æ˜¯ç”¨äºå­˜å‚¨å†å²è®°å¿†çš„ã€‚ç”±äºchat modelé€šå¸¸æ¥å—messageåˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¸”è¾“å‡ºä¸€ä¸ªmessageï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è®©stateæ¥ä¿å­˜å†å²messagesï¼Œå³ç»´æŠ¤ä¸€ä¸ªmessages stateã€‚å¯ä»¥ä½¿ç”¨LangGraphå†…ç½®çš„MessagesStateç±»æ¥å®ç°è¿™ä¸ªéœ€æ±‚ã€‚\n",
    "- è‡³äºå…·ä½“çš„æŒä¹…åŒ–æ–¹å¼ï¼Œç®€å•èµ·è§ï¼Œè¿™é‡Œæˆ‘ä»¬é€‰æ‹©æŒä¹…åŒ–åˆ°å†…å­˜ï¼Œå¯ä»¥ä½¿ç”¨MemorySaverï¼Œè¿™æ˜¯ä¸€ä¸ªin-memory checkpoint saverï¼Œéœ€è¦åœ¨graphç¼–è¯‘æ—¶ä¼ å…¥ã€‚\n",
    "\n",
    "\n",
    "\n",
    "æŒ‰ç…§æ­¥éª¤ï¼Œæˆ‘ä»¬\n",
    "1. å®šä¹‰ä¸€ä¸ªStateGraphï¼Œå¹¶å°†è¯¥graphçš„stateå®šä¹‰ä¸ºmessageåˆ—è¡¨ï¼ˆç”¨MessagesStateï¼‰ï¼›\n",
    "2. å®šä¹‰chat modelçš„è°ƒç”¨å‡½æ•°call_modelï¼Œè¿™ä¸ªå‡½æ•°è¾“å…¥æ˜¯ä¸€ä¸ªstateï¼Œç„¶åå°†stateä¸­çš„å…·ä½“æ•°æ®ï¼ˆè¿™é‡Œæ˜¯history messagesï¼‰ä½œä¸ºè¾“å…¥ä¼ ç»™chat modelï¼Œè¿”å›modelçš„è¾“å‡ºï¼›\n",
    "3. å¾€graphä¸­æ·»åŠ ä¸€ä¸ªnodeï¼Œè¿™ä¸ªnodeå°†ä¼šè°ƒç”¨call_modelï¼›\n",
    "4. ç”¨ä¸€ä¸ªå†…å­˜ä¸­çš„checkpointeræ¥ç¼–è¯‘graphï¼Œä»è€Œåœ¨æ¯æ¬¡è¿è¡Œæ—¶å­˜å‚¨messagesã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªStateGraphï¼Œå¹¶å°†è¯¥graphçš„stateå®šä¹‰ä¸ºmessageåˆ—è¡¨ï¼ˆç”¨MessagesStateï¼‰ï¼›\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# å®šä¹‰è°ƒç”¨chatçš„å‡½æ•°ï¼Œæ ¹æ®è¾“å…¥çš„stateï¼ˆæœ¬è´¨ä¸Šæ˜¯messageåˆ—è¡¨ï¼‰ï¼Œè°ƒç”¨chat modelå¹¶è¿”å›responseï¼›\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    # Update message history with response:\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# å¾€graphä¸­æ·»åŠ ä¸€ä¸ªnodeï¼Œè¿™ä¸ªnodeå°†ä¼šè°ƒç”¨chat modelï¼›\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# ç”¨ä¸€ä¸ªå†…å­˜ä¸­çš„checkpointeræ¥ç¼–è¯‘graphï¼Œä»è€Œåœ¨æ¯æ¬¡è¿è¡Œæ—¶å­˜å‚¨messagesã€‚\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“æˆ‘ä»¬è¿è¡Œè¯¥appæ—¶ï¼Œæˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªæŒ‡å®šthread_idçš„é…ç½®å­—å…¸dictã€‚æ­¤IDç”¨äºåŒºåˆ†ä¼šè¯çº¿ç¨‹ï¼ˆä¾‹å¦‚ï¼Œåœ¨ä¸åŒç”¨æˆ·ä¹‹é—´ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ŒRetroï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï½ ä»Šå¤©æœ‰ä»€ä¹ˆæƒ³å’Œæˆ‘åˆ†äº«ï¼Œæˆ–è€…éœ€è¦å¸®å¿™çš„åœ°æ–¹å—ï¼Ÿæ— è®ºæ˜¯é—®é¢˜ã€è¯é¢˜ï¼Œè¿˜æ˜¯éšä¾¿èŠèŠï¼Œæˆ‘éƒ½åœ¨è¿™é‡Œå“¦ï¼ ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"aaa666\"}}\n",
    "query = \"ä½ å¥½ï¼Œæˆ‘å«Retro\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ çš„åå­—æ˜¯ **Retro** å‘€ï¼ğŸ˜„ æœ‰æ²¡æœ‰ä»€ä¹ˆæƒ³å’Œæˆ‘èŠèŠçš„ï¼Ÿæ— è®ºæ˜¯ç”Ÿæ´»ã€å…´è¶£ï¼Œè¿˜æ˜¯ä»»ä½•é—®é¢˜ï¼Œéƒ½å¯ä»¥å°½ç®¡è¯´å“¦ï½\n"
     ]
    }
   ],
   "source": [
    "query = \"æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆ?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "ä½ å¥½ï¼Œæˆ‘å«Retro\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ŒRetroï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï½ ä»Šå¤©æœ‰ä»€ä¹ˆæƒ³å’Œæˆ‘åˆ†äº«ï¼Œæˆ–è€…éœ€è¦å¸®å¿™çš„åœ°æ–¹å—ï¼Ÿæ— è®ºæ˜¯é—®é¢˜ã€è¯é¢˜ï¼Œè¿˜æ˜¯éšä¾¿èŠèŠï¼Œæˆ‘éƒ½åœ¨è¿™é‡Œå“¦ï¼ ğŸ˜Š\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ çš„åå­—æ˜¯ **Retro** å‘€ï¼ğŸ˜„ æœ‰æ²¡æœ‰ä»€ä¹ˆæƒ³å’Œæˆ‘èŠèŠçš„ï¼Ÿæ— è®ºæ˜¯ç”Ÿæ´»ã€å…´è¶£ï¼Œè¿˜æ˜¯ä»»ä½•é—®é¢˜ï¼Œéƒ½å¯ä»¥å°½ç®¡è¯´å“¦ï½\n"
     ]
    }
   ],
   "source": [
    "for message in output[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯¥graphçš„stateå†å²å¯ä»¥é€šè¿‡`.get_state`å¾—åˆ°ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "ä½ å¥½ï¼Œæˆ‘å«Retro\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ŒRetroï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï½ ä»Šå¤©æœ‰ä»€ä¹ˆæƒ³å’Œæˆ‘åˆ†äº«ï¼Œæˆ–è€…éœ€è¦å¸®å¿™çš„åœ°æ–¹å—ï¼Ÿæ— è®ºæ˜¯é—®é¢˜ã€è¯é¢˜ï¼Œè¿˜æ˜¯éšä¾¿èŠèŠï¼Œæˆ‘éƒ½åœ¨è¿™é‡Œå“¦ï¼ ğŸ˜Š\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ çš„åå­—æ˜¯ **Retro** å‘€ï¼ğŸ˜„ æœ‰æ²¡æœ‰ä»€ä¹ˆæƒ³å’Œæˆ‘èŠèŠçš„ï¼Ÿæ— è®ºæ˜¯ç”Ÿæ´»ã€å…´è¶£ï¼Œè¿˜æ˜¯ä»»ä½•é—®é¢˜ï¼Œéƒ½å¯ä»¥å°½ç®¡è¯´å“¦ï½\n"
     ]
    }
   ],
   "source": [
    "state = app.get_state(config).values\n",
    "\n",
    "for message in state[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
