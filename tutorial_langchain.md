# Tutorial of LangChain

> ref: https://python.langchain.com/docs/tutorials/
> å‰ç½®çŸ¥è¯†ï¼šLLMï¼ŒPromptï¼ŒEmbeddingï¼ŒRAGç­‰åŸºç¡€æ¦‚å¿µ

## 1. Chat Model å’Œ Prompt Template
é¦–å…ˆæˆ‘ä»¬ç†Ÿæ‚‰ä¸‹Langchainä¸­å°è£…çš„LLMæ¨¡å‹å’ŒPromptæ¨¡æ¿çš„ä½¿ç”¨ï¼Œè¿™æ˜¯åé¢æ„å»ºLLMåº”ç”¨çš„åŸºç¡€ä¸­çš„åŸºç¡€ï¼Œå³å¦‚ä½•å‘ä¸€ä¸ªLLMæä¾›åŸºç¡€çš„ä¸Šä¸‹æ–‡è¾“å…¥ã€‚

### 1.1 å®šä¹‰ä¸€ä¸ªchatæ¨¡å‹ï¼ˆinit_chat_modelï¼‰


åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ langchain æä¾›çš„å°è£…æ–¹æ³• init_chat_model æ¥æ„å»ºä¸€ä¸ªå¯ç”¨çš„èŠå¤©æ¨¡å‹å®ä¾‹ã€‚é€šè¿‡åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼ˆå¦‚ OPENAI_API_KEY å’Œ OPENAI_API_BASEï¼‰ï¼Œå®ç°äº†ä¸æ¨¡å‹æœåŠ¡çš„åŠ¨æ€è¿æ¥ã€‚æœ¬ç¤ºä¾‹ä¸­é‡‡ç”¨çš„æ˜¯å…¼å®¹ OpenAI æ¥å£æ ¼å¼çš„æ˜Ÿç«å¤§æ¨¡å‹ï¼ˆx1ï¼‰ï¼Œä¾¿äºç›´æ¥å¤ç”¨ OpenAI çš„ API è°ƒç”¨é€»è¾‘ï¼Œä»è€Œå®ç°æ— ç¼æ¥å…¥å›½äº§å¤§æ¨¡å‹æœåŠ¡ã€‚


```python
from langchain.chat_models import init_chat_model
import os
from dotenv import load_dotenv
# è¯»å– .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡, æˆ‘ä»¬å¯ä»¥åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEYï¼Œæ–‡ä»¶å†…å®¹ç±»ä¼¼äºï¼š OPENAI_API_KEY=your_api_key
load_dotenv() 
api_key = os.getenv("OPENAI_API_KEY")
base_url = os.getenv('OPENAI_API_BASE')

# è°ƒç”¨langchainçš„å°è£…init_chat_modelæ–¹æ³•ï¼Œåˆå§‹åŒ–ä¸€ä¸ªchatæ¨¡å‹
# è¿™é‡Œæˆ‘ç”¨çš„æ˜¯æ˜Ÿç«x1æ¨¡å‹ï¼Œx1å…¼å®¹openaiçš„è°ƒç”¨æ ¼å¼ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥è°ƒç”¨openaiçš„æ¥å£
# base_urlå’Œapi_keyæ˜¯ä».env æ–‡ä»¶ä¸­è¯»å–çš„, æ¥è‡ªæ˜Ÿç«å¼€æ”¾å¹³å°
model = init_chat_model(model = "x1", model_provider="openai",
                        base_url=base_url, api_key=api_key)
```

### 1.2 ç”¨Messageæ„å»ºè¾“å…¥prompt
Langchainä¸­çš„ä¸€ä¸ªå¸¸è§çš„è¾“å…¥promptçš„æ–¹å¼æ˜¯æ„å»ºä¸€ä¸ªmessagesåˆ—è¡¨ï¼ˆé‡Œé¢ä¸€èˆ¬æ˜¯ä¸€äº›Messageå¯¹è±¡ï¼‰ï¼Œç„¶åå°†messagesä½œä¸ºå‚æ•°ä¼ å…¥invoke

å…¶å®ç›´æ¥åœ¨invokeä¸­ä¼ å…¥å­—ç¬¦ä¸²æˆ–è€…openaiæ ¼å¼çš„jsonä¹Ÿæ˜¯å¯ä»¥çš„


```python
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘å®¶ï¼Œæ ¹æ®è¾“å…¥çš„ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡"),
    HumanMessage(content="ä½ å¥½ï¼Œç°åœ¨æ˜¯å‘¨äº”çš„æ™šä¸Šï¼Œæˆ‘åœ¨å­¦ä¹ langchainã€‚"),
]

AIMessage = model.invoke(messages)
print(AIMessage)
```

    content="Hello, it's currently Friday night, and I'm studying LangChain." additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 529, 'prompt_tokens': 23, 'total_tokens': 552, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': None, 'system_fingerprint': None, 'id': None, 'service_tier': None, 'finish_reason': None, 'logprobs': None} id='run--d75838c7-e3ab-4974-9c77-cc6b440e5c30-0' usage_metadata={'input_tokens': 23, 'output_tokens': 529, 'total_tokens': 552, 'input_token_details': {}, 'output_token_details': {}}


### 1.3 ç”¨Prompt templatesç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡

è™½ç„¶å¯ä»¥ç›´æ¥ç”¨Messageå¯¹è±¡æ¥æ„å»ºpromptè¾“å…¥ï¼Œä½†ä¸€èˆ¬ä¼šä½¿ç”¨Prompt Templatesæ¥åšMessageæ ¼å¼çš„é¢„å®šä¹‰ã€‚

åœ¨Langchainä¸­ï¼ŒPrompt Templates ç”¨äºå°†ç”¨æˆ·è¾“å…¥å’Œå‚æ•°è½¬åŒ–ä¸ºå¯¹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤ã€‚è¿™å¯ä»¥ç”¨æ¥å¼•å¯¼æ¨¡å‹çš„å“åº”ï¼Œå¸®åŠ©å…¶ç†è§£ä¸Šä¸‹æ–‡ï¼Œå¹¶ç”Ÿæˆç›¸å…³ä¸”è¿è´¯çš„è¯­è¨€è¾“å‡ºã€‚

- Prompt Templates çš„è¾“å…¥æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­æ¯ä¸ªé”®ä»£è¡¨æç¤ºæ¨¡æ¿ä¸­éœ€è¦å¡«å……çš„ä¸€ä¸ªå˜é‡ã€‚
- Prompt Templates çš„è¾“å‡ºæ˜¯ä¸€ä¸ª PromptValueã€‚è¿™ä¸ª PromptValue å¯ä»¥ä¼ é€’ç»™ LLM æˆ– ChatModelï¼Œä¹Ÿå¯ä»¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨ã€‚
- PromptValue çš„è®¾è®¡ç›®çš„æ˜¯ä¸ºäº†æ–¹ä¾¿åœ¨å­—ç¬¦ä¸²å’Œæ¶ˆæ¯ä¹‹é—´è¿›è¡Œåˆ‡æ¢ã€‚

#### 1.3.1 PromptTemplate

Prompt templatesæœ‰å¾ˆå¤šä¸åŒçš„ç±»ï¼ŒPromptTemplateæ˜¯æœ€ç®€å•çš„ä¸€ä¸ªï¼š


```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("Tell me a joke about {topic}")
# prompt_template = PromptTemplate.from_file("prompt_template.txt")  # ä¹Ÿå¯ä»¥ä»æ–‡ä»¶åŠ è½½
prompt_value = prompt_template.invoke({"topic": "cats"})

print(type(prompt_value))
print("-------------")
print(prompt_value)
print("-------------")
print(prompt_value.to_string())
print("-------------")
print(prompt_value.to_messages())
```

    <class 'langchain_core.prompt_values.StringPromptValue'>
    -------------
    text='Tell me a joke about cats'
    -------------
    Tell me a joke about cats
    -------------
    [HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})]



```python
model.invoke(prompt_value)
```




    AIMessage(content='Why did the cat become a detective?  \nBecause it was good at **pawing** through clues! ğŸ˜¸', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 741, 'prompt_tokens': 6, 'total_tokens': 747, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': None, 'system_fingerprint': None, 'id': None, 'service_tier': None, 'finish_reason': None, 'logprobs': None}, id='run--f2105269-f4c7-4331-a63f-c309e19c187b-0', usage_metadata={'input_tokens': 6, 'output_tokens': 741, 'total_tokens': 747, 'input_token_details': {}, 'output_token_details': {}})



#### 1.3.2 ChatPromptTemplates

ChatPromptTemplates ç”¨äºæ ¼å¼åŒ–ä¸€ç»„æ¶ˆæ¯ï¼ˆmessagesï¼‰ã€‚è¿™äº›â€œæ¨¡æ¿â€æœ¬èº«ç”±å¤šä¸ªå­æ¨¡æ¿ç»„æˆã€‚æ„å»ºå’Œä½¿ç”¨ ChatPromptTemplate çš„å¸¸è§æ–¹å¼å¦‚ä¸‹ï¼š


```python
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate

prompt_template = ChatPromptTemplate([
    SystemMessagePromptTemplate.from_template("You are a helpful assistant that translates {input_language} to {output_language}."),
    HumanMessagePromptTemplate.from_template("{sentence}")
])

prompt_value = prompt_template.invoke({
    "input_language": "English", 
    "output_language": "French", 
    "sentence": "I love programming."
    })

print(type(prompt_value))
print("-------------")
print(prompt_value)
print("-------------")
print(prompt_value.to_string())
print("-------------")
print(prompt_value.to_messages())
```

    <class 'langchain_core.prompt_values.ChatPromptValue'>
    -------------
    messages=[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]
    -------------
    System: You are a helpful assistant that translates English to French.
    Human: I love programming.
    -------------
    [SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]



```python
ai_message = model.invoke(prompt_value)
print(ai_message)
print(ai_message.content)
```

    content='**French Translation:**  \n**"J\'aime programmer."**  \n\nThis translates directly while preserving the active sense of enjoying the activity. If emphasizing enthusiasm, you could also say:  \n**"J\'adore programmer!"** (*"I adore programming!"*)  \n\nBoth are natural and context-appropriate. ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 703, 'prompt_tokens': 16, 'total_tokens': 719, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': None, 'system_fingerprint': None, 'id': None, 'service_tier': None, 'finish_reason': None, 'logprobs': None} id='run--6c6f0e1f-b0c1-4f96-a867-59d2d1f20ad8-0' usage_metadata={'input_tokens': 16, 'output_tokens': 703, 'total_tokens': 719, 'input_token_details': {}, 'output_token_details': {}}
    **French Translation:**  
    **"J'aime programmer."**  
    
    This translates directly while preserving the active sense of enjoying the activity. If emphasizing enthusiasm, you could also say:  
    **"J'adore programmer!"** (*"I adore programming!"*)  
    
    Both are natural and context-appropriate. ğŸ˜Š


#### 1.3.3 MessagesPlaceholder
è¿™ä¸ªæç¤ºæ¨¡æ¿çš„ä½œç”¨æ˜¯åœ¨ç‰¹å®šä½ç½®æ’å…¥ä¸€ç»„æ¶ˆæ¯ã€‚åœ¨ä¸Šé¢çš„ ChatPromptTemplate ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¦‚ä½•æ ¼å¼åŒ–ä¸¤æ¡æ¶ˆæ¯ï¼Œå…¶ä¸­æ¯ä¸ªæ¶ˆæ¯éƒ½æ˜¯ä¸€ä¸ªMessagePromptTemplateå¯¹è±¡ï¼Œå¯ä»¥è®©ç”¨æˆ·ä¼ å…¥ç‰¹å®šçš„å­—ç¬¦ä¸²å‚æ•°æ¯”å¦‚{input_language}ã€‚ä½†å¦‚æœæˆ‘ä»¬å¸Œæœ›ç”¨æˆ·ç›´æ¥ä¼ å…¥ä¸€ç»„æ¶ˆæ¯ï¼Œå¹¶å°†å®ƒä»¬æ’å…¥åˆ°æç¤ºä¸­çš„æŸä¸ªä½ç½®ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿè¿™æ—¶å°±å¯ä»¥ä½¿ç”¨ MessagesPlaceholderã€‚


```python
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage,SystemMessage
prompt_template = ChatPromptTemplate([
    MessagesPlaceholder(variable_name="history"),
])
history=[
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘æœºå™¨äººï¼Œå°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡"),
    HumanMessage(content="ä½ å¥½")
]
prompt_value = prompt_template.invoke(
    {"history":history}
)

print(type(prompt_value))
print("-------------")
print(prompt_value)
print("-------------")
print(prompt_value.to_string())
print("-------------")
print(prompt_value.to_messages())
```

    <class 'langchain_core.prompt_values.ChatPromptValue'>
    -------------
    messages=[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘æœºå™¨äººï¼Œå°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ å¥½', additional_kwargs={}, response_metadata={})]
    -------------
    System: ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘æœºå™¨äººï¼Œå°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡
    Human: ä½ å¥½
    -------------
    [SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘æœºå™¨äººï¼Œå°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ å¥½', additional_kwargs={}, response_metadata={})]


## 2. å¤–éƒ¨æ•°æ®è¿æ¥ä¸æ£€ç´¢
è¿™ä¸€ç« ä»‹ç» LangChain ä¸­çš„æ–‡æ¡£åŠ è½½å™¨ï¼ˆ[document loader](https://python.langchain.com/docs/concepts/document_loaders/)ï¼‰ã€åµŒå…¥ï¼ˆ[embedding](https://python.langchain.com/docs/concepts/embedding_models/)ï¼‰å’Œå‘é‡å­˜å‚¨ï¼ˆvector storeï¼‰è¿™å‡ ä¸ªæŠ½è±¡æ¨¡å—ã€‚è¿™äº›æ¨¡å—çš„è®¾è®¡ç›®çš„æ˜¯æ”¯æŒä»ï¼ˆå‘é‡ï¼‰æ•°æ®åº“æˆ–å…¶ä»–æ•°æ®æºä¸­æ£€ç´¢æ•°æ®ï¼Œå¹¶å°†å…¶æ•´åˆåˆ° LLM çš„å·¥ä½œæµç¨‹ä¸­ã€‚å®ƒä»¬å¯¹äºéœ€è¦åœ¨æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­è·å–å¹¶åˆ©ç”¨å¤–éƒ¨æ•°æ®çš„åº”ç”¨éå¸¸é‡è¦ï¼Œæ¯”å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å°±æ˜¯ä¸€ä¸ªå…¸å‹æ¡ˆä¾‹

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åŸºäºä¸€ä¸ª PDF æ–‡æ¡£æ„å»ºä¸€ä¸ªç®€å•çš„æœ¬åœ°æ–‡æ¡£å†…å®¹æ£€ç´¢å¼•æ“ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„æŸ¥è¯¢ï¼Œåœ¨ PDF ä¸­æ£€ç´¢å‡ºç›¸ä¼¼çš„å†…å®¹ç‰‡æ®µã€‚

```bash
pip install langchain-community pypdf
```

### 2.1 æ–‡æ¡£ï¼ˆDocumentsï¼‰
[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)æ˜¯LangChainä¸­çš„ä¸€ä¸ªç”¨äºå­˜å‚¨æ–‡æœ¬æ®µåŠå…¶å…ƒæ•°æ®çš„ç±»ï¼Œä¸€ä¸ª Document å¯¹è±¡é€šå¸¸ä»£è¡¨çš„æ˜¯ä¸€ä¸ªè¾ƒå¤§æ–‡æ¡£ä¸­çš„ä¸€ä¸ªç‰‡æ®µï¼ˆchunkï¼‰ã€‚
Documentç±»çš„ä¸»è¦å±æ€§åŒ…æ‹¬ï¼š
- `page_content`ï¼šæ–‡æœ¬æ®µçš„å†…å®¹ã€‚
- `metadata`ï¼šä¸æ–‡æœ¬æ®µç›¸å…³çš„å…ƒæ•°æ®ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå¯ä»¥ç”¨äºè®°å½•æ–‡æ¡£çš„æ¥æºã€ä¸å…¶ä»–æ–‡æ¡£ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠå…¶ä»–ç›¸å…³ä¿¡æ¯ã€‚ã€‚
- `id(å¯é€‰)`ï¼šæ–‡æœ¬æ®µçš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚


```python

from langchain_core.documents import Document

documents = [
    Document(
        page_content="Dogs are great companions, known for their loyalty and friendliness.",
        metadata={"source": "mammal-pets-doc"},
    ),
    Document(
        page_content="Cats are independent pets that often enjoy their own space.",
        metadata={"source": "mammal-pets-doc"},
    ),
]
```

### 2.2 åŠ è½½æ–‡æ¡£ï¼ˆdocument_loadersï¼‰
LangChainçš„ç¤¾åŒºç”Ÿæ€ä¸­ï¼Œé›†æˆäº†å¾ˆå¤šæ–‡æ¡£åŠ è½½æ–¹å¼ï¼ŒåŒ…æ‹¬PDFã€Wordã€CSVç­‰ï¼Œè¿™é‡Œæˆ‘ä»¬ä»¥PDFä¸ºä¾‹ï¼Œä»‹ç»ä¸‹document_loadersä¸­PyPDFLoaderçš„ä½¿ç”¨ã€‚
å…¶ä»–ç±»å‹å¯å‚è€ƒï¼šhttps://python.langchain.com/docs/integrations/document_loaders/

PyPDFLoader ä¼šå°†æ¯ä¸€é¡µ PDF åŠ è½½ä¸ºä¸€ä¸ª Document å¯¹è±¡ã€‚å¯¹äºæ¯ä¸ªå¯¹è±¡ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥è½»æ¾è®¿é—®ä»¥ä¸‹å†…å®¹ï¼š
- è¯¥é¡µçš„å­—ç¬¦ä¸²å†…å®¹ï¼›
- åŒ…å«æ–‡ä»¶åå’Œé¡µç çš„å…ƒæ•°æ®ã€‚


```python
from langchain_community.document_loaders import PyPDFLoader
file_path = "data/è®ºæ–‡é€å®¡ä¸ç­”è¾©è§„å®š.pdf"
loader = PyPDFLoader(file_path)

docs = loader.load() # docs æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª Document å¯¹è±¡

print(len(docs))
for doc in docs:
    print(doc.page_content[:100],"...........")
    print(doc.metadata)
    print("----------------")
```

    5
    è®¡ç®—æœºå­¦é™¢å…³äºç¡•å£«ç ”ç©¶ç”Ÿå­¦ä½è®ºæ–‡é€å®¡ä¸ç­”è¾©çš„è‹¥å¹²è§„å®š
    ï¼ˆè¯•è¡Œï¼‰
    ä¸ºä¿è¯ç ”ç©¶ç”Ÿå­¦ä½æˆäºˆè´¨é‡ï¼Œæ ¹æ®ã€Šæ­å·ç”µå­ç§‘æŠ€å¤§å­¦åšå£«ã€ç¡•å£«å­¦ä½æˆäºˆå·¥ä½œç»†åˆ™ã€‹ï¼ˆæ­
    ç”µç ”[2021]165 å·ï¼‰å’Œã€Šæ­å·ç”µå­ç§‘æŠ€å¤§å­¦å…³äºç ” ...........
    {'producer': '', 'creator': 'WPS æ–‡å­—', 'creationdate': '2022-04-11T14:27:44+06:27', 'author': 'Admin', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2022-04-11T14:27:44+06:27', 'sourcemodified': "D:20220411142744+06'27'", 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/è®ºæ–‡é€å®¡ä¸ç­”è¾©è§„å®š.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}
    ----------------
    ï¼ˆäºŒï¼‰ä¸Šä¸€å¹´åº¦ï¼Œå…¶å¯¼å¸ˆæ‰€æŒ‡å¯¼çš„ç¡•å£«ç ”ç©¶ç”Ÿç›²å®¡ç»“æœå¹³å‡åˆ†ä¸º 2.5 åŠä»¥ä¸‹çš„ï¼›
    ï¼ˆä¸‰ï¼‰æˆªè‡³æœ¬å¹´åº¦ï¼Œå…¶å¯¼å¸ˆå°šæœªæœ‰æŒ‡å¯¼ç¡•å£«ç ”ç©¶ç”Ÿæ¯•ä¸šçš„ã€‚
    ï¼ˆå››ï¼‰æœ¬äººå­¦ä½è®ºæ–‡åˆæ¬¡ç”³è¯·é€å®¡æ—¶å› é™¢å†…è¯„å®¡ç»“æœä¸ä½³è€Œè¢«ç»ˆæ­¢é€å®¡ç¨‹åºçš„ ...........
    {'producer': '', 'creator': 'WPS æ–‡å­—', 'creationdate': '2022-04-11T14:27:44+06:27', 'author': 'Admin', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2022-04-11T14:27:44+06:27', 'sourcemodified': "D:20220411142744+06'27'", 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/è®ºæ–‡é€å®¡ä¸ç­”è¾©è§„å®š.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}
    ----------------
    ç¡•å£«å­¦ä½è®ºæ–‡é€å®¡ç¨‹åºç»ˆæ­¢åï¼Œç”³è¯·äººé¡»æ ¹æ®è¯„é˜…æ„è§ä¹¦å¯¹å­¦ä½è®ºæ–‡è¿›è¡Œå®è´¨æ€§ä¿®æ”¹ï¼Œ3
    ä¸ªæœˆå 1 å¹´å†…æŒ‰ç¨‹åºé‡æ–°ç”³è¯·é€å®¡ã€‚
    ç¬¬åä¸€æ¡ æ¶‰åŠå›½å®¶ç§˜å¯†ï¼ˆå†›äº‹å®‰å…¨ä¿¡æ¯ï¼‰çš„è®ºæ–‡ï¼Œä¸¥æ ¼æŒ‰ç…§ã€Šæ­å·ç”µå­ç§‘æŠ€å¤§å­¦ç ”ç©¶ç”Ÿ
    ä» ...........
    {'producer': '', 'creator': 'WPS æ–‡å­—', 'creationdate': '2022-04-11T14:27:44+06:27', 'author': 'Admin', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2022-04-11T14:27:44+06:27', 'sourcemodified': "D:20220411142744+06'27'", 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/è®ºæ–‡é€å®¡ä¸ç­”è¾©è§„å®š.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}
    ----------------
    ä¸ªæœˆå 1 å¹´å†…æŒ‰ç¨‹åºé‡æ–°é€å®¡ã€‚ç¬¬äºŒæ¬¡ç›²å®¡ä»ç„¶ä¸é€šè¿‡ï¼Œä¸å†å—ç†å…¶ç­”è¾©ç”³è¯·ã€‚
    ç¬¬åä¸ƒæ¡ ç¡•å£«ç ”ç©¶ç”Ÿå­¦ä½è®ºæ–‡é¦–æ¬¡ç›²å®¡è¯„é˜…æ„è§å« C æˆ– D çš„ï¼Œé¡»åˆ—å…¥ç‹¬ç«‹ç­”è¾©ç»„ç­”è¾©ã€‚
    ç¬¬åå…«æ¡ ç‹¬ç«‹ç­”è¾©ç»„çš„è¯„å®¡å·¥ä½œç”±å­¦é™¢ ...........
    {'producer': '', 'creator': 'WPS æ–‡å­—', 'creationdate': '2022-04-11T14:27:44+06:27', 'author': 'Admin', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2022-04-11T14:27:44+06:27', 'sourcemodified': "D:20220411142744+06'27'", 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/è®ºæ–‡é€å®¡ä¸ç­”è¾©è§„å®š.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}
    ----------------
    å†…å¤–å­¦æœ¯æœŸåˆŠæŠ•ç¨¿å¹¶å‘è¡¨è®ºæ–‡ï¼›åœ¨ CCF æ¨èä¼šè®®åˆ—è¡¨çš„ C ç±»ä¼šè®®ä¸Šå‘è¡¨è®ºæ–‡ç­‰æ•ˆäºå‘ EI æ”¶
    å½•çš„å›½å†…å¤–å­¦æœ¯æœŸåˆŠæŠ•ç¨¿å¹¶å‘è¡¨è®ºæ–‡ã€‚
    ç¬¬äºŒåäº”æ¡ æœ¬è§„å®šä» 2019 çº§ç¡•å£«ç ”ç©¶ç”Ÿå¼€å§‹æ‰§è¡Œã€‚
    ç¬¬äºŒåå…­æ¡  ...........
    {'producer': '', 'creator': 'WPS æ–‡å­—', 'creationdate': '2022-04-11T14:27:44+06:27', 'author': 'Admin', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2022-04-11T14:27:44+06:27', 'sourcemodified': "D:20220411142744+06'27'", 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/è®ºæ–‡é€å®¡ä¸ç­”è¾©è§„å®š.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}
    ----------------


### 2.3 æ–‡æ¡£çš„è‡ªå®šä¹‰åˆ†å‰²ï¼ˆText Splittingï¼‰
ç›´æ¥ä½¿ç”¨PyPDFLoaderå¯ä»¥è·å¾—ä¸€ä¸ªç®€å•çš„æ–‡æ¡£åˆ†å‰²ï¼Œå³æ¯ä¸€é¡µéƒ½æ˜¯ä¸€ä¸ªDocumentã€‚ä½†æ˜¯å¯¹äºä¿¡æ¯æ£€ç´¢å’Œä¸‹æ¸¸é—®ç­”ä»»åŠ¡æ¥è¯´ï¼Œç›´æ¥ä»¥é¡µé¢ä¸ºå•ä½å¯èƒ½è¿‡äºç²—ç•¥ã€‚å› ä¸ºæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ ¹æ®è¾“å…¥æŸ¥è¯¢æ£€ç´¢åˆ°èƒ½å›ç­”é—®é¢˜çš„ Document å¯¹è±¡ï¼Œæ‰€ä»¥æœ‰å¿…è¦å°† PDF è¿›ä¸€æ­¥æ‹†åˆ†ï¼Œé¿å…æ–‡ä¸­ç›¸å…³å†…å®¹è¢«ä¸Šä¸‹æ–‡ç¨€é‡Šï¼Œä»è€Œæå‡æ£€ç´¢æ•ˆæœã€‚

Langchainå°è£…äº†ä¸€ç³»åˆ—æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆ[text splitter](https://python.langchain.com/docs/concepts/text_splitters/#document-structured-based)ï¼‰ï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„åˆ†å‰²å™¨ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„æ–‡æœ¬åˆ†å‰²å™¨ï¼š

1. **CharacterTextSplitter**ï¼šåŸºäºå­—ç¬¦çš„åˆ†å‰²å™¨ï¼Œå°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ¯æ®µå›ºå®šé•¿åº¦çš„å­—ç¬¦ã€‚
2. **TokenTextSplitter**ï¼šåŸºäºæ ‡è®°ï¼ˆtokenï¼‰çš„åˆ†å‰²å™¨ï¼Œå°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ¯æ®µå›ºå®šæ•°é‡çš„æ ‡è®°ã€‚
3. **RecursiveCharacterTextSplitter**ï¼šé€’å½’åˆ†å‰²å™¨ï¼Œæ ¹æ®å¸¸è§çš„åˆ†éš”ç¬¦ï¼ˆå¦‚æ¢è¡Œç¬¦ï¼‰å°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ®µè½ã€‚
4. **SpacyTextSplitter**ï¼šåŸºäº Spacy çš„åˆ†å‰²å™¨ï¼Œä½¿ç”¨ Spacy è¿›è¡Œæ–‡æœ¬åˆ†å‰²ã€‚
5. **NLTKTextSplitter**ï¼šåŸºäº NLTK çš„åˆ†å‰²å™¨ï¼Œä½¿ç”¨ NLTK è¿›è¡Œæ–‡æœ¬åˆ†å‰²ã€‚
6. **MarkdownHeaderTextSplitter**ï¼šåŸºäº Markdown æ ‡é¢˜çš„åˆ†å‰²å™¨ï¼Œå°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ¯ä¸ªæ ‡é¢˜ä½œä¸ºä¸€ä¸ªæ®µè½ã€‚

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨RecursiveCharacterTextSplitterä½œä¸ºåˆ†å‰²å™¨ï¼ŒRecursiveCharacterTextSplitter æ˜¯ LangChain ä¸­å¸¸ç”¨çš„æ–‡æœ¬åˆ†å‰²å·¥å…·ï¼Œä¸»è¦ç”¨äºå°†è¾ƒé•¿çš„æ–‡æœ¬åˆ‡åˆ†æˆæ›´å°çš„ç‰‡æ®µï¼Œä»¥ä¾¿åç»­å¤„ç†ï¼ˆå¦‚å‘é‡åŒ–ã€æ£€ç´¢ç­‰ï¼‰ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€’å½’åœ°æŒ‰ç…§æŒ‡å®šçš„åˆ†éš”ç¬¦ï¼ˆå¦‚æ¢è¡Œç¬¦ã€å¥å·ã€é€—å·ç­‰ï¼‰è¿›è¡Œåˆ†å‰²ï¼Œä¼˜å…ˆä½¿ç”¨è¾ƒå¤§çš„åˆ†éš”ç¬¦ï¼Œå¦‚æœåˆ†å‰²åç‰‡æ®µä»ç„¶è¿‡é•¿ï¼Œåˆ™ç»§ç»­ç”¨æ›´å°çš„åˆ†éš”ç¬¦é€’å½’åˆ†å‰²ï¼Œç›´åˆ°æ¯ä¸ªç‰‡æ®µéƒ½ä¸è¶…è¿‡è®¾å®šçš„æœ€å¤§é•¿åº¦ã€‚

ä¸»è¦å‚æ•°åŒ…æ‹¬ï¼š

- chunk_size ï¼šæ¯ä¸ªåˆ†ç‰‡çš„æœ€å¤§å­—ç¬¦æ•°ã€‚
- chunk_overlap ï¼šç›¸é‚»åˆ†ç‰‡ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œä¿è¯ä¸Šä¸‹æ–‡è¿ç»­æ€§ã€‚
- separators ï¼šåˆ†å‰²æ—¶ä¼˜å…ˆä½¿ç”¨çš„åˆ†éš”ç¬¦åˆ—è¡¨ï¼ˆå¦‚["\n\n", "\n", "ã€‚", "ï¼Œ", " "]ï¼‰ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾ç½® add_start_index=Trueï¼Œè¿™æ ·æ¯ä¸ªåˆ†æ®µåœ¨åŸå§‹æ–‡æ¡£ä¸­çš„èµ·å§‹å­—ç¬¦ä½ç½®å°†ä¼šä½œä¸º start_index æ·»åŠ åˆ°è¯¥æ®µçš„å…ƒæ•°æ®ä¸­ã€‚


```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500, chunk_overlap=100, add_start_index=True
)
all_splits = text_splitter.split_documents(docs)

print(len(all_splits))
print(type(all_splits[0]))
print(all_splits[0].page_content)
print("------------")
print(all_splits[1].page_content)
```

    12
    <class 'langchain_core.documents.base.Document'>
    è®¡ç®—æœºå­¦é™¢å…³äºç¡•å£«ç ”ç©¶ç”Ÿå­¦ä½è®ºæ–‡é€å®¡ä¸ç­”è¾©çš„è‹¥å¹²è§„å®š
    ï¼ˆè¯•è¡Œï¼‰
    ä¸ºä¿è¯ç ”ç©¶ç”Ÿå­¦ä½æˆäºˆè´¨é‡ï¼Œæ ¹æ®ã€Šæ­å·ç”µå­ç§‘æŠ€å¤§å­¦åšå£«ã€ç¡•å£«å­¦ä½æˆäºˆå·¥ä½œç»†åˆ™ã€‹ï¼ˆæ­
    ç”µç ”[2021]165 å·ï¼‰å’Œã€Šæ­å·ç”µå­ç§‘æŠ€å¤§å­¦å…³äºç ”ç©¶ç”Ÿå­¦ä½è®ºæ–‡ç›²å®¡å·¥ä½œçš„è§„å®šï¼ˆè¯•è¡Œï¼‰ã€‹ï¼ˆæ­
    ç”µç ”[2021]164 å·ï¼‰ï¼Œç»“åˆæˆ‘é™¢å…·ä½“æƒ…å†µï¼Œç‰¹åˆ¶å®šæœ¬è§„å®šã€‚
    ç¬¬ä¸€ç«  å­¦ä½è®ºæ–‡çš„é€å®¡ç”³è¯·
    ç¬¬ä¸€æ¡ å‡¡æ”»è¯»æœ¬é™¢ç¡•å£«å­¦ä½çš„ç ”ç©¶ç”Ÿï¼Œåœ¨è§„å®šçš„å­¦ä¹ æœŸé™å†…ï¼Œä¿®å®Œæœ¬äººåŸ¹å…»è®¡åˆ’ä¸­çš„å…¨
    éƒ¨è¯¾ç¨‹ï¼Œæˆç»©åˆæ ¼ï¼Œè¾¾åˆ°æ‰€è§„å®šçš„æ€»å­¦åˆ†ï¼Œå–å¾—ç›¸åº”çš„ç§‘æŠ€æˆæœï¼Œå®Œæˆç¡•å£«å­¦ä½è®ºæ–‡çš„ç ”ç©¶
    å’Œæ’°å†™å·¥ä½œï¼Œç»å¯¼å¸ˆå®¡é˜…é€šè¿‡åï¼Œæ–¹å¯å‘å­¦é™¢ç”³è¯·ç¡•å£«å­¦ä½è®ºæ–‡ç›²å®¡é€å®¡ã€‚
    ç¬¬äºŒæ¡ å­¦ä½è®ºæ–‡å®è¡ŒåŒç›²è¯„å®¡ï¼Œé€å®¡çš„ç ”ç©¶ç”Ÿå­¦ä½è®ºæ–‡é¡»éšå»ä½œè€…å’Œå¯¼å¸ˆçš„ç›¸å…³ä¿¡æ¯ï¼Œ
    åŒæ—¶åé¦ˆçš„è¯„é˜…ç»“æœé¡»éšå»è¯„é˜…äººçš„ä¿¡æ¯ï¼Œä»¥ä¿è¯è®ºæ–‡è¯„é˜…çš„å®¢è§‚å…¬æ­£ã€‚
    ç¬¬ä¸‰æ¡ è®ºæ–‡æäº¤è¦æ±‚ï¼š
    ï¼ˆä¸€ï¼‰å­¦ç”Ÿç”³è¯·å­¦ä½è®ºæ–‡ç›²å®¡é€å®¡æ—¶é¡»å¡«å†™ã€Šç ”ç©¶ç”Ÿå­¦ä½è®ºæ–‡é€å®¡èµ„æ ¼å®¡æŸ¥è¡¨ã€‹ï¼Œå¯¼å¸ˆåŒ
    æ„é€å®¡å¹¶å†™å‡ºè¯„é˜…æ„è§ã€‚
    ï¼ˆäºŒï¼‰ç›²å®¡å­¦ä½è®ºæ–‡åº”ä¸º PDF æ ¼å¼æ–‡æ¡£ï¼Œç¬¦åˆç›²å®¡å­¦ä½è®ºæ–‡æ ¼å¼è¦æ±‚ï¼Œå‘½åè§„åˆ™ï¼šâ€œå­¦ç”Ÿ
    å­¦å·_å­¦ç”Ÿå§“å_å­¦ä½è®ºæ–‡é¢˜ç›®â€ã€‚
    ------------
    æ„é€å®¡å¹¶å†™å‡ºè¯„é˜…æ„è§ã€‚
    ï¼ˆäºŒï¼‰ç›²å®¡å­¦ä½è®ºæ–‡åº”ä¸º PDF æ ¼å¼æ–‡æ¡£ï¼Œç¬¦åˆç›²å®¡å­¦ä½è®ºæ–‡æ ¼å¼è¦æ±‚ï¼Œå‘½åè§„åˆ™ï¼šâ€œå­¦ç”Ÿ
    å­¦å·_å­¦ç”Ÿå§“å_å­¦ä½è®ºæ–‡é¢˜ç›®â€ã€‚
    ï¼ˆä¸‰ï¼‰ç›²å®¡è®ºæ–‡æ ¼å¼ï¼šå­¦ä½è®ºæ–‡ä¸»ä½“éƒ¨åˆ†ï¼ˆä¸­è‹±æ–‡æ‘˜è¦ã€ç›®å½•ã€æ­£æ–‡ã€é™„å½•ã€å‚è€ƒæ–‡çŒ®ï¼‰
    å¿…é¡»å®Œæ•´ï¼Œä¸èƒ½æ•…æ„åˆ é™¤æˆ–éšç’ï¼Œé™„å½•ä¸­å¦‚æœ‰æ¶‰åŠæœ¬äººã€å¯¼å¸ˆçš„æ•æ„Ÿæ–‡å­—å¯ä»¥ç”¨â€œ*â€æ›¿æ¢å¤„
    ç†ã€‚å°é¢ã€åŸåˆ›å£°æ˜ã€æˆæƒå£°æ˜ã€è‡´è°¢ç­‰éä¸»ä½“éƒ¨åˆ†ï¼Œå­¦æœ¯ä¸ç«¯æ£€æµ‹å®¹æ˜“å‡ºç°é‡å¤ï¼Œå­¦ç”Ÿè‡ª
    è¡Œåˆ é™¤ï¼Œç„¶åç»Ÿä¸€é‡‡ç”¨ç›²å®¡ä¸“ç”¨è®ºæ–‡å°é¢ã€‚
    ç¬¬å››æ¡ è®¡ç®—æœºå­¦é™¢è¦æ±‚æœ¬é™¢ç¡•å£«ç ”ç©¶ç”Ÿå°†æ’°å†™å®Œæ•´çš„å­¦ä½è®ºæ–‡é€’äº¤å¯¼å¸ˆç¬¬ä¸€æ¬¡å®¡é˜…çš„
    æ—¶é—´æœ€æ™šæˆªæ­¢è‡³é€å®¡æ—¥ä¹‹å‰30 æ—¥ï¼Œè®ºæ–‡ä¿®æ”¹åé€’äº¤å¯¼å¸ˆæœ€åä¸€æ¬¡å®¡é˜…çš„æ—¶é—´æœ€æ™šæˆªæ­¢è‡³é€å®¡
    æ—¥ä¹‹å‰ 10 æ—¥ï¼Œè¶…è¿‡æ­¤è§„å®šæ—¶é—´ä»æœªå°†è®ºæ–‡é€’äº¤å¯¼å¸ˆå®¡é˜…è€…ï¼Œå¯¼å¸ˆæœ‰æƒåœ¨ã€Šè®ºæ–‡é€å®¡èµ„æ ¼å®¡æŸ¥
    è¡¨ã€‹ä¸­æ‰¹å¤ä¸åŒæ„é€å®¡ã€‚
    ç¬¬äº”æ¡ è®¡ç®—æœºå­¦é™¢æå€¡æ‰€æœ‰ç¡•å£«å­¦ä½è®ºæ–‡å®è¡Œé¢„ç­”è¾©åˆ¶ã€‚å¯¹äºå‡ºç°ä¸‹åˆ—æƒ…å†µä¹‹ä¸€çš„ç¡•å£«
    ç ”ç©¶ç”Ÿå¿…é¡»å‚åŠ ç”±å­¦é™¢ç»Ÿä¸€ç»„ç»‡çš„é¢„ç­”è¾©ã€‚
    ï¼ˆä¸€ï¼‰ä¸Šä¸€å¹´åº¦ï¼Œå…¶å¯¼å¸ˆæ‰€æŒ‡å¯¼çš„ç¡•å£«ç ”ç©¶ç”Ÿç›²å®¡ç»“æœå‡ºç°â€œCã€Câ€ã€â€œCã€Dâ€æˆ–â€œDã€
    Dâ€çš„ï¼›


å…³äºå¤„ç† PDF çš„è¯¦ç»†å†…å®¹ï¼ˆä¾‹å¦‚æå–ç‰¹å®šç« èŠ‚æˆ–å›¾ç‰‡ä¸­çš„æ–‡æœ¬ï¼‰ï¼Œå¯ä»¥å‚è€ƒ [How to load PDFs](https://python.langchain.com/docs/how_to/document_loader_pdf/)

### 2.4 Embeddings


ä¸Šä¸€èŠ‚ä¸­æˆ‘ä»¬è¯»å–äº†æ–‡æ¡£å¹¶åˆ’åˆ†æˆäº†è‹¥å¹²æ–‡æœ¬chunkï¼Œä½†æ˜¯å½“æˆ‘ä»¬è¦æ„å»ºå¤§å‹çŸ¥è¯†åº“æ—¶ï¼Œå¾€å¾€éœ€è¦å°†è¿™äº›æ–‡æœ¬æ•°æ®è½¬åŒ–ä¸ºæ•°å€¼å‘é‡ï¼ˆé€šè¿‡embedding modelï¼‰ï¼Œæ–¹ä¾¿åç»­åº”ç”¨ä¸­åšå¯¹çŸ¥è¯†åº“åšå‘é‡æœç´¢ã€‚

**å‘é‡æœç´¢**æ˜¯ä¸€ç§å¸¸è§çš„æ–¹å¼ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢éç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚éç»“æ„åŒ–æ–‡æœ¬ï¼‰ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ–‡æœ¬è¡¨ç¤ºä¸ºæ•°å€¼å‘é‡è¿›è¡Œå­˜å‚¨ã€‚æŸ¥è¯¢æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¾“å…¥ä¹Ÿè½¬åŒ–ä¸ºåŒç»´åº¦çš„å‘é‡ï¼Œå¹¶é€šè¿‡å‘é‡ç›¸ä¼¼åº¦åº¦é‡ï¼ˆå¦‚ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰æ¥æ‰¾åˆ°ç›¸å…³æ–‡æœ¬ã€‚

LangChain å°è£…äº†å¾ˆå¤šä¸åŒçš„embeddingæ¨¡å‹ï¼Œå¯æŸ¥è¯¢ï¼šhttps://python.langchain.com/docs/integrations/text_embedding/ã€‚æˆ–è€…ä¹Ÿå¯ä»¥ä»¿ç…§è¿™äº›å°è£…å»è‡ªå®šä¹‰ä¸€äº›embeddingæ¨¡å‹ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªçŸ¥åçš„[bge](https://huggingface.co/BAAI/bge-m3) embeddingæ¨¡å‹ï¼Œå¯ä»¥ä»huggingfaceä¸Šä¸‹è½½ï¼Œæ›´å¤šçš„æ¨¡å‹å¯ä»¥å‚è€ƒ[embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard)ã€‚


```python
from langchain_community.embeddings import HuggingFaceBgeEmbeddings

model_name = "models/bge-m3"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': True}
embedding_model_bge = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

vector_1 = embedding_model_bge.embed_query(all_splits[0].page_content)
vector_2 = embedding_model_bge.embed_query(all_splits[1].page_content)
assert len(vector_1) == len(vector_2)
print(f"Generated vectors of length {len(vector_1)}\n")
print(vector_1[:10])

```

    Generated vectors of length 1024
    
    [-0.09491164982318878, -0.0020208219066262245, 0.024046452715992928, -0.019713258370757103, -0.008063230663537979, -0.03716067597270012, -0.006405872758477926, -0.030813977122306824, 0.010731630958616734, 0.01573730818927288]


### 2.5 æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆVectorStoreï¼‰
å‘é‡æ•°æ®åº“æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡ç”¨äºé«˜æ•ˆå­˜å‚¨ã€ç´¢å¼•å’ŒæŸ¥è¯¢é«˜ç»´å‘é‡æ•°æ®çš„æ•°æ®åº“ç³»ç»Ÿã€‚ä¸ä¼ ç»Ÿæ•°æ®åº“ä¸åŒï¼Œå®ƒä¸ä¾èµ–äºç²¾ç¡®åŒ¹é…æˆ–é¢„å®šä¹‰æ ‡å‡†ï¼Œè€Œæ˜¯åŸºäºå‘é‡è·ç¦»æˆ–ç›¸ä¼¼æ€§è¿›è¡Œæœç´¢å’Œæ£€ç´¢ã€‚
ä¸»æµå‘é‡æ•°æ®åº“ç±»å‹ï¼š
1. Chromaï¼šè½»é‡çº§å¼€æºå‘é‡æ•°æ®åº“ï¼Œç®€å•æ˜“ç”¨ï¼Œé€‚åˆæœ¬åœ°å¼€å‘å’Œå°å‹é¡¹ç›®
2. FAISSï¼šFacebookå¼€å‘çš„é«˜æ€§èƒ½ç›¸ä¼¼æ€§æœç´¢åº“ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®é›†
3. Pineconeï¼šå®Œå…¨æ‰˜ç®¡çš„å‘é‡æ•°æ®åº“æœåŠ¡ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ
4. Weaviateï¼šå¼€æºå‘é‡æœç´¢å¼•æ“ï¼Œæ”¯æŒå‘é‡æœç´¢å’ŒçŸ¥è¯†å›¾è°±
5. Qdrantï¼šäº‘åŸç”Ÿå‘é‡æ•°æ®åº“æœåŠ¡ï¼Œæä¾›é«˜æ•ˆAPIæ¥å£

ä¹‹å‰æˆ‘ä»¬ä»‹ç»äº†æ€ä¹ˆå°†æ–‡æœ¬æˆ–è€…æ–‡æ¡£è½¬æ¢æˆå‘é‡ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ä»¥FAISSä¸ºä¾‹ä»‹ç»æ€ä¹ˆç”¨å‘é‡æ•°æ®åº“æ¥å­˜å‚¨å¹¶æ£€ç´¢è¿™äº›å‘é‡ã€‚

LangChain çš„ VectorStore ä¾›äº†ç»Ÿä¸€çš„APIæ¥ä¸ä¸åŒçš„åº•å±‚å‘é‡æ•°æ®åº“äº¤äº’ï¼Œå…·ä½“æ¥è¯´æä¾›äº†ç”¨äºæ·»åŠ æ–‡æœ¬å’Œ Document å¯¹è±¡çš„æ–¹æ³•ï¼Œå¹¶æ”¯æŒä½¿ç”¨å¤šç§ç›¸ä¼¼åº¦åº¦é‡æ–¹å¼è¿›è¡ŒæŸ¥è¯¢ã€‚è¿™ç±»å¯¹è±¡é€šå¸¸éœ€è¦ä¸embeddingsæ¨¡å‹é…åˆä½¿ç”¨ï¼Œä»¥ç¡®å®šå¦‚ä½•å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ•°å€¼å‘é‡ã€‚





```python
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS

embedding_dim = len(embedding_model_bge.embed_query("hello world"))
index = faiss.IndexFlatL2(embedding_dim)

# å®ä¾‹åŒ–ä¸€ä¸ªvecotr store
vector_store = FAISS(
    embedding_function=embedding_model_bge, # æŒ‡å®šembeddingæ¨¡å‹
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={},
)

```


```python
# ä¸ºå‘é‡æ•°æ®åº“æ·»åŠ æ–‡æ¡£æ•°æ®
ids = vector_store.add_documents(documents=all_splits)
print(ids)

```

    ['b188bfed-4b5c-4196-bdc1-c4d7453807fb', 'd3e63b1a-adb3-4bfb-abe0-ae782f304488', '529cef49-26bf-40ba-b4ff-3ad3860ffbe5', '776c864a-222a-4d0b-a487-6af9b30dea98', '36835fe8-4f93-4414-ab54-3971d98e0217', 'c94c5015-3fb5-4da1-adab-2ca6e713111a', '1f842869-28e7-44e2-8f86-fb18d6a49bcd', '907bc740-ac92-4301-bc8a-a9cc21b96c0f', 'c36d8245-46ea-4fc4-8dfb-d7453127bf7c', '00120957-e3a9-43c3-b460-a9701585d6c6', 'b4872df8-5c91-48c8-901b-5acd3b0786d4', 'b30a635d-fb6a-483f-9930-65303c2eb4aa']



```python
results = vector_store.similarity_search(
    "ä¸“å®¶è¯„å®¡æ„è§æœ‰å“ªå‡ ä¸ªç­‰çº§"
)
print(len(results))
print("-------------")
print(results[0].page_content)
print("-------------")
print(results[0].metadata)

```

    4
    -------------
    ä¸Šé™¢å†…è¯„å®¡ä¸“å®¶å®¡é˜…ï¼Œå¯¹è®ºæ–‡å†™ä½œè§„èŒƒæ€§æ˜¯å¦å·²è¾¾åˆ°ç›¸å…³å­¦ä½çš„å­¦æœ¯æ°´å¹³ï¼Œèƒ½å¦å‚åŠ é€å®¡ç­‰
    ç»™å‡ºæ˜ç¡®è¯„é˜…æ„è§ã€‚
    ä¸“å®¶è¯„é˜…æ„è§åˆ†ä¸ºï¼šAï¼åŒæ„é€å®¡ï¼›Bï¼åŒæ„ç»è¿‡å°çš„ä¿®æ”¹åé€å®¡ï¼ˆä¸å†è¿›è¡Œé™¢å†…è¯„å®¡ï¼‰ï¼›
    Cï¼éœ€è¦è¿›è¡Œè¾ƒå¤§çš„ä¿®æ”¹ï¼Œæš‚ç¼“é€å®¡ï¼ˆ3 æ—¥å†…ä¿®æ”¹åé€åŸä¸“å®¶å†å®¡ï¼‰ï¼›Dï¼æœªè¾¾åˆ°å­¦ä½è®ºæ–‡è¦æ±‚ï¼Œ
    ä¸åŒæ„é€å®¡ï¼ˆè§†ä¸ºâ€œå­˜åœ¨å¼‚è®®â€ï¼Œè‡ªåŠ¨åŠ é€ 1 ä½ä¸“å®¶é™¢å†…è¯„å®¡ï¼‰ã€‚
    ç¬¬åæ¡ æ ¹æ®é™¢å†…è¯„å®¡ç»“æœçš„å…·ä½“æƒ…å†µï¼Œå°†å¯¹é€å®¡ç”³è¯·ä½œå¦‚ä¸‹å¤„ç†ï¼š
    ï¼ˆä¸€ï¼‰è¯„é˜…æ„è§ä¸ºâ€œCã€Câ€ã€â€œCã€Dâ€æˆ–â€œDã€Dâ€æ—¶ï¼Œæœ¬æ¬¡é€å®¡ç¨‹åºç»ˆæ­¢ã€‚
    ï¼ˆäºŒï¼‰è¯„é˜…æ„è§ä¸ºâ€œDã€Aâ€æˆ–â€œDã€Bâ€æ—¶ï¼Œç”³è¯·äººåŠå¯¼å¸ˆé¡»ç­¾ç½²ã€Šè®¡ç®—æœºå­¦é™¢å­¦ä½è®ºæ–‡
    è´¨é‡æ‰¿è¯ºä¹¦ã€‹åæ–¹èƒ½é€å®¡ï¼Œå¦åˆ™æœ¬æ¬¡é€å®¡ç¨‹åºç»ˆæ­¢ã€‚
    ï¼ˆä¸‰ï¼‰å…¶ä½™æƒ…å†µçš†è§†ä¸ºæ— å¼‚è®®ï¼ŒåŒæ„é€å®¡ã€‚
    -------------
    {'producer': '', 'creator': 'WPS æ–‡å­—', 'creationdate': '2022-04-11T14:27:44+06:27', 'author': 'Admin', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2022-04-11T14:27:44+06:27', 'sourcemodified': "D:20220411142744+06'27'", 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/è®ºæ–‡é€å®¡ä¸ç­”è¾©è§„å®š.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'start_index': 794}


## 3. ç»“æ„åŒ–è¾“å‡ºï¼ˆStructured outputsï¼‰

åœ¨æŸäº›åº”ç”¨åœºæ™¯é‡Œï¼Œæˆ‘ä»¬éœ€è¦è®©LLMè¾“å‡ºç»“æ„åŒ–çš„æ•°æ®ï¼Œæ¯”å¦‚JSONæˆ–è€…XMLã€‚LangChainæä¾›äº†ä¸€äº›å·¥å…·æ¥å¸®åŠ©æˆ‘ä»¬å®ç°è¿™ä¸ªç›®æ ‡ã€‚
   

### 3.1 with_structured_output()
æ¯”å¦‚ï¼ŒLangChain æä¾›äº†ä¸€ä¸ªåä¸º `with_structured_output()` çš„æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨å®Œæˆå°†æ¨¡å¼ï¼ˆschemaï¼‰ç»‘å®šåˆ°æ¨¡å‹ä»¥åŠè§£æè¾“å‡ºçš„æ•´ä¸ªè¿‡ç¨‹ã€‚å¯¹äºæ‰€æœ‰æ”¯æŒç»“æ„åŒ–è¾“å‡ºçš„æ¨¡å‹æä¾›å•†ï¼Œè¿™ä¸ªè¾…åŠ©å‡½æ•°éƒ½æ˜¯å¯ç”¨çš„ï¼Œå…·ä½“å¯å‚è€ƒï¼šhttps://python.langchain.com/docs/concepts/structured_outputs/ï¼Œå¤§è‡´å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š


```python
# Define schema
schema = ...
# Bind schema to model
model_with_structure = model.with_structured_output(schema)
# Invoke the model to produce structured output that matches the schema
structured_output = model_with_structure.invoke(user_input)
```

### 3.2  prompting+è¾“å‡ºè§£æå™¨ï¼ˆOutput Parserï¼‰
ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒ `.with_structured_output()`ï¼Œå› ä¸ºå¹¶ä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½å…·å¤‡tool calling åŠŸèƒ½æˆ– JSON æ¨¡å¼æ”¯æŒã€‚å¯¹äºè¿™ç±»æ¨¡å‹ï¼Œéœ€è¦ç›´æ¥é€šè¿‡æç¤ºï¼ˆpromptï¼‰å¼•å¯¼æ¨¡å‹ä»¥ç‰¹å®šæ ¼å¼è¾“å‡ºï¼Œç„¶åä½¿ç”¨è¾“å‡ºè§£æå™¨ï¼ˆoutput parserï¼‰ä»æ¨¡å‹åŸå§‹è¾“å‡ºä¸­æå–ç»“æ„åŒ–çš„ç»“æœã€‚

ä»¥ä¸‹ç¤ºä¾‹ä½¿ç”¨å†…ç½®çš„ PydanticOutputParser æ¥è§£æä¸€ä¸ªè¢«æç¤ºç”Ÿæˆç¬¦åˆæŒ‡å®š Pydantic æ¨¡å¼ï¼ˆschemaï¼‰çš„èŠå¤©æ¨¡å‹çš„è¾“å‡ºã€‚
åœ¨ä¸€èˆ¬çš„æ¨¡å‹é—®ç­”åŸºç¡€ä¸Šï¼Œä¸»è¦åšäº†ä¸¤ä»¶äº‹ï¼š
1. ä½¿ç”¨parseçš„get_format_instructions()æ–¹æ³•ï¼Œå°† format_instructions ç›´æ¥æ·»åŠ åˆ°promptä¸­
2. ç„¶ååœ¨æ¨¡å‹ç”Ÿæˆå›ç­”åï¼Œè®©parserè¿›ä¸€æ­¥è§£ææ¨¡å‹çš„å›ç­”


```python
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from pydantic import BaseModel, Field

# å®šä¹‰ä¸€ä¸ªpydanticç±»
class Person(BaseModel):
    """ä¸ªäººä¿¡æ¯"""

    name: str = Field(description="å§“å")
    height_in_meters: float = Field( description="èº«é«˜")

# æ ¹æ®pydanticç±»å®šä¹‰ä¸€ä¸ªè¾“å‡ºè§£æå™¨
parser = PydanticOutputParser(pydantic_object=Person)

# ç”¨parseå¯¹åº”çš„format_instructionsæ„å»ºprompt
prompt_template = ChatPromptTemplate([
    SystemMessagePromptTemplate.from_template("å›ç­”ç”¨æˆ·, ç”¨jsonæ ¼å¼è¾“å‡º\n{format_instructions}"),
    HumanMessagePromptTemplate.from_template("{sentence}")
]).partial(format_instructions=parser.get_format_instructions())
sentence = "Anna is 23 years old and she is 6 feet tall"
prompt = prompt_template.invoke({"sentence": sentence})
print(prompt)
print("--------------------------")

# è°ƒç”¨llmæ¨¡å‹
output = model.invoke(prompt)
print(output)
print("--------------------------")

# è§£æè¾“å‡ºä¸ºä¸€ä¸ªPersonå¯¹è±¡
result = parser.parse(output.content)
print(type(result))
print(result)
print("--------------------------")
```

    messages=[SystemMessage(content='å›ç­”ç”¨æˆ·, ç”¨jsonæ ¼å¼è¾“å‡º\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}\nthe object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{"description": "ä¸ªäººä¿¡æ¯", "properties": {"name": {"description": "å§“å", "title": "Name", "type": "string"}, "height_in_meters": {"description": "èº«é«˜", "title": "Height In Meters", "type": "number"}}, "required": ["name", "height_in_meters"]}\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='Anna is 23 years old and she is 6 feet tall', additional_kwargs={}, response_metadata={})]
    --------------------------
    content='```json\n{\n  "name": "Anna",\n  "height_in_meters": 1.8288\n}\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 604, 'prompt_tokens': 244, 'total_tokens': 848, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': None, 'system_fingerprint': None, 'id': None, 'service_tier': None, 'finish_reason': None, 'logprobs': None} id='run--f0479681-60cf-4f15-b4f1-6fd8137ae49d-0' usage_metadata={'input_tokens': 244, 'output_tokens': 604, 'total_tokens': 848, 'input_token_details': {}, 'output_token_details': {}}
    --------------------------
    <class '__main__.Person'>
    name='Anna' height_in_meters=1.8288
    --------------------------


JsonOutputParserä¹Ÿæ˜¯ä¸€ä¸ªå¸¸ç”¨çš„parserï¼š


```python
from langchain_core.output_parsers import JsonOutputParser
parser = JsonOutputParser(pydantic_object=Person)
prompt_template = ChatPromptTemplate([
    SystemMessagePromptTemplate.from_template("å›ç­”ç”¨æˆ·, ç”¨jsonæ ¼å¼è¾“å‡º\n{format_instructions}"),
    HumanMessagePromptTemplate.from_template("{sentence}")
]).partial(format_instructions=parser.get_format_instructions())
sentence = "Anna is 23 years old and she is 6 feet tall"
prompt = prompt_template.invoke({"sentence": sentence})
print(prompt)
print("--------------------------")

# è°ƒç”¨llmæ¨¡å‹
output = model.invoke(prompt)
print(output)
print("--------------------------")

# è§£æè¾“å‡ºä¸ºä¸€ä¸ªdictå­—å…¸å¯¹è±¡
result = parser.parse(output.content)
print(type(result))
print(result)
print("--------------------------")
```

    messages=[SystemMessage(content='å›ç­”ç”¨æˆ·, ç”¨jsonæ ¼å¼è¾“å‡º\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}\nthe object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{"description": "ä¸ªäººä¿¡æ¯", "properties": {"name": {"description": "å§“å", "title": "Name", "type": "string"}, "height_in_meters": {"description": "èº«é«˜", "title": "Height In Meters", "type": "number"}}, "required": ["name", "height_in_meters"]}\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='Anna is 23 years old and she is 6 feet tall', additional_kwargs={}, response_metadata={})]
    --------------------------
    content='```json\n{\n  "name": "Anna",\n  "height_in_meters": 1.8288\n}\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 567, 'prompt_tokens': 244, 'total_tokens': 811, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': None, 'system_fingerprint': None, 'id': None, 'service_tier': None, 'finish_reason': None, 'logprobs': None} id='run--1a179c73-fe9d-46e6-a3c4-aa660c9a7ee5-0' usage_metadata={'input_tokens': 244, 'output_tokens': 567, 'total_tokens': 811, 'input_token_details': {}, 'output_token_details': {}}
    --------------------------
    <class 'dict'>
    {'name': 'Anna', 'height_in_meters': 1.8288}
    --------------------------


é™¤æ­¤ä¹‹å¤–ï¼Œä¸ºäº†ä¿æŒchainå®šä¹‰æ ¼å¼çš„ä¸€è‡´æ€§å’Œå¯ç»„åˆæ€§ï¼Œå³ä½¿ä¸éœ€è¦ç»“æ„åŒ–è¾“å‡ºï¼Œä¹Ÿå¯ä»¥ç”¨StrOutputParseræ¥ä½œä¸ºchainçš„ç»“å°¾ç»„ä»¶ã€‚

## 4. LangChain Expression Language (LCEL)
> **LCEL** (**L**ang**C**hain **E**xpression **L**anguage) æ˜¯ LangChain æ¨å‡ºçš„ä¸€ä¸ªâ€œè¡¨è¾¾å¼è¯­è¨€â€ï¼Œä¸“ä¸ºæ„å»ºã€ç»„åˆå’Œè¿è¡Œé“¾å¼ç»„ä»¶è€Œè®¾è®¡ã€‚å®ƒç”¨ä¸€ç§å£°æ˜å¼ã€å¯ç»„åˆçš„æ–¹å¼æ¥æè¿°é“¾å¼ä»»åŠ¡ï¼Œè®©ä½ æ›´å®¹æ˜“æ„å»ºå¤æ‚çš„ LLM åº”ç”¨ã€‚

### 4.1 LCEL çš„æ ¸å¿ƒï¼šRunnable
Runnable æ˜¯æ‰€æœ‰å¯è¢«â€œæ‰§è¡Œâ€çš„ç»„ä»¶çš„åŸºç¡€æ¥å£ã€‚å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆâ€œå¯è¿è¡Œçš„å‡½æ•°â€ï¼Œå®ƒæ¥æ”¶ä¸€ä¸ªè¾“å…¥ï¼Œè¿”å›ä¸€ä¸ªè¾“å‡ºï¼Œå¹¶ä¸”å¯ä»¥è¢«ç»„åˆã€è°ƒç”¨ã€æ‰¹é‡æ‰§è¡Œã€å¼‚æ­¥è°ƒç”¨ã€æˆ–æµå¼è¾“å‡ºã€‚æœ‰äº† Runnableï¼Œå°±å¯ä»¥æŠŠå„ç§åŠŸèƒ½ç»„ä»¶åƒâ€œä¹é«˜ç§¯æœ¨â€ä¸€æ ·ç»„åˆèµ·æ¥ï¼Œæ„å»ºéå¸¸å¤æ‚çš„è¯­è¨€æ¨¡å‹åº”ç”¨ï¼Œè€Œä¸éœ€è¦å†™å¾ˆå¤šæ§åˆ¶é€»è¾‘ã€‚åœ¨ LCEL ä¸­ï¼Œä¸€åˆ‡ç»„ä»¶éƒ½å®ç°äº† Runnable æ¥å£ã€‚

ä»»ä½•å®ç°äº† Runnable çš„å¯¹è±¡éƒ½å…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š
- Invokedï¼ˆè°ƒç”¨ï¼‰ï¼šå•ä¸ªè¾“å…¥è¢«è½¬æ¢ä¸ºä¸€ä¸ªè¾“å‡ºã€‚
- Batchedï¼ˆæ‰¹å¤„ç†ï¼‰ï¼šå¤šä¸ªè¾“å…¥è¢«é«˜æ•ˆåœ°è½¬æ¢ä¸ºå¤šä¸ªè¾“å‡ºã€‚
- Streamedï¼ˆæµå¼è¾“å‡ºï¼‰ï¼šè¾“å‡ºä¼šåœ¨ç”Ÿæˆçš„åŒæ—¶è¢«æµå¼ä¼ è¾“å‡ºæ¥ã€‚
- Inspectedï¼ˆå¯æ£€æŸ¥ï¼‰ï¼šå¯ä»¥è®¿é—®å…³äº Runnable çš„è¾“å…¥ã€è¾“å‡ºå’Œé…ç½®çš„ç»“æ„ä¿¡æ¯ã€‚
- Composedï¼ˆå¯ç»„åˆï¼‰ï¼šå¤šä¸ª Runnable å¯ä»¥ä½¿ç”¨LCELç»„åˆåœ¨ä¸€èµ·ï¼Œæ„å»ºå¤æ‚çš„å¤„ç†æµç¨‹ã€‚




```python
from langchain_core.runnables import RunnableLambda

runnable_to_str = RunnableLambda(lambda x: str(x))

runnable_to_str.invoke(547) # å¸¸è§„è°ƒç”¨
runnable_to_str.batch([7, 8, 9]) # æ‰¹é‡è°ƒç”¨
runnable_to_str.stream([10, 11, 12]) # æµå¼è°ƒç”¨
```

LCELå…è®¸å¯¹å¤šä¸ªrunnableåšç»„åˆï¼ˆcomositionï¼‰ï¼Œäº‹å®ä¸Šï¼ŒLCEL chain å°±æ˜¯é€šè¿‡ç»„åˆç°æœ‰çš„ Runnable æ„å»ºçš„ã€‚å…¶ä¸¤ä¸ªä¸»è¦çš„ç»„åˆåŸè¯­æ˜¯ï¼š
- RunnableSequenceï¼šè¡¨ç¤ºæŒ‰é¡ºåºæ‰§è¡Œçš„ä¸€ç³»åˆ— Runnableï¼Œæ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºä¼šä½œä¸ºä¸‹ä¸€ä¸ªæ­¥éª¤çš„è¾“å…¥ã€‚
- RunnableParallelï¼šè¡¨ç¤ºå¹¶è¡Œæ‰§è¡Œçš„å¤šä¸ª Runnableï¼Œæ‰€æœ‰æ­¥éª¤åŒæ—¶æ¥æ”¶ç›¸åŒçš„è¾“å…¥ï¼Œå¹¶åˆ†åˆ«ç”Ÿæˆå„è‡ªçš„è¾“å‡ºã€‚

ä¸€ä¸ªRunnableSequenceç®€å•ä¾‹å­ï¼š


```python
# RunnableSequence example
from langchain_core.runnables import RunnableLambda, RunnableSequence
runnable1 = RunnableLambda(lambda x: x + 1)
runnable2 = RunnableLambda(lambda x: x + 2)
chain = RunnableSequence(runnable1, runnable2)
chain.invoke(0)
```




    3



æ¥ä¸‹æ¥çœ‹ä¸€ä¸ªRunnableParallelçš„ç®€å•ä¾‹å­ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒRunnableParallelçš„è¾“å…¥æ˜¯ä¸€ä¸ªdictï¼Œkeyä»£è¡¨æŸä¸ªrunnableçš„åå­—ï¼Œvalueåˆ™æ˜¯runnableæœ¬èº«ã€‚è°ƒç”¨RunnableParallelåï¼Œè¾“å‡ºçš„è¿˜æ˜¯ä¸€ä¸ªdictï¼Œå¹¶ä¸”keyå’Œè¾“å…¥çš„keyæ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œåªæ˜¯valueå˜æˆäº†å¯¹åº”runnableçš„è¾“å‡ºã€‚


```python
# RunnableParallel example
from langchain_core.runnables import RunnableLambda, RunnableParallel
runnable1 = RunnableLambda(lambda x: x + 1)
runnable2 = RunnableLambda(lambda x: x + 2)
chain = RunnableParallel({"a": runnable1, "b": runnable2})
chain.invoke(0)

```




    {'a': 1, 'b': 2}



ä¸¤è€…ä¸€èµ·ç”¨çš„ä¾‹å­


```python
# RunnableSequence & RunnableParallel example:
from langchain_core.runnables import RunnableLambda, RunnableSequence, RunnableParallel
runnable1 = RunnableLambda(lambda x: x + 1)
runnable2 = RunnableLambda(lambda x: x + 2)
runnable3 = RunnableLambda(lambda x: x["a"] * x["b"])
chain = RunnableSequence(RunnableParallel({"a": runnable1, "b": runnable2}), runnable3)
chain.invoke(1)
# output: (1+1) * (1+2) = 6

```




    6



### 4.2 LCEL çš„è¯­æ³•ç³–ï¼š| æ“ä½œç¬¦

`RunnableSequence`å’Œ`RunnableParallel`çš„ä½¿ç”¨éå¸¸æ™®éï¼Œå› æ­¤LangChainä¸ºå®ƒä»¬åˆ›å»ºäº†ä¸€ç§é€Ÿè®°è¯­æ³•ã€‚è¿™æœ‰åŠ©äºä½¿ä»£ç æ›´å…·å¯è¯»æ€§å’Œç®€æ´æ€§ã€‚
ç®€å•æ¥è¯´ï¼ŒLCEL é€šè¿‡æ“ä½œç¬¦é‡è½½ï¼Œå®ç°äº†ç»„ä»¶ä¹‹é—´çš„ç»„åˆï¼Œæœ€å¸¸è§çš„å°±æ˜¯ç”¨ï½œæ¥ä»£æ›¿`RunnableParallel`ï¼Œå³ï½œä¹‹é—´çš„ç»„ä»¶ä¼šè¢«ç»„åˆæˆä¸€ä¸ª`RunnableParallel`ã€‚


```python
# RunnableSequence example
from langchain_core.runnables import RunnableLambda, RunnableSequence
runnable1 = RunnableLambda(lambda x: x + 1)
runnable2 = RunnableLambda(lambda x: x + 2)
# chain = RunnableSequence(runnable1, runnable2)
chain = runnable1 | runnable2  # | ç›¸å½“äºRunnableSequence
chain.invoke(0)
```




    3



åœ¨LCELè¡¨è¾¾å¼å†…éƒ¨ï¼Œå­—å…¸dictä¼šè‡ªåŠ¨è½¬æ¢ä¸ºRunnableParallelã€‚ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œdictåªæ˜¯åœ¨LCELçš„chainä¸­è¢«è½¬æ¢äº†ï¼Œdictæœ¬èº«å¹¶ä¸æ˜¯RunnableParallelï¼Œæ‰€ä»¥ä¸èƒ½å•ç‹¬å¯¹ä¸€ä¸ªdictç›´æ¥åšinvokeï¼Œæ¯”å¦‚{"a": runnable1, "b": runnable2}.invoke()æ˜¯ä¸åˆæ³•çš„ã€‚


```python
# RunnableSequence & RunnableParallel example:
from langchain_core.runnables import RunnableLambda, RunnableSequence, RunnableParallel
runnable1 = RunnableLambda(lambda x: x + 1)
runnable2 = RunnableLambda(lambda x: x + 2)
runnable3 = RunnableLambda(lambda x: x["a"] * x["b"])
chain = {"a": runnable1, "b": runnable2} | runnable3

chain.invoke(1)
# output: (1+1) * (1+2) = 6

```




    6



æœ€åå…³äºLCELéœ€è¦æçš„ä¸€ç‚¹æ˜¯ï¼Œè™½ç„¶ç¡®å®æœ‰ç”¨æˆ·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒåŒ…å«æ•°ç™¾ä¸ªæ­¥éª¤çš„é“¾ï¼Œä½†é€šå¸¸å»ºè®®å°† LCEL ç”¨äºè¾ƒä¸ºç®€å•çš„ç¼–æ’ä»»åŠ¡ã€‚è€Œå¦‚æœåº”ç”¨åœºæ™¯æ¶‰åŠå¤æ‚çš„çŠ¶æ€ç®¡ç†ã€åˆ†æ”¯ã€å¾ªç¯æˆ–å¤šä¸ªæ™ºèƒ½ä½“ï¼ˆagentï¼‰ï¼ŒLangChainæ¨èç”¨æˆ·ä½¿ç”¨ [LangGraph](https://python.langchain.com/docs/concepts/architecture/#langgraph)ï¼Œå®ƒæ›´é€‚åˆå¤„ç†æ­¤ç±»å¤æ‚æµç¨‹ã€‚

## 5. LangGraph
LangGraph æ˜¯ LangChain ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ‰©å±•åº“ï¼Œä¸“ä¸ºæ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæˆ–è€…å¤æ‚å·¥ä½œæµè®¾è®¡ã€‚å®ƒå¯ä»¥é€šè¿‡å›¾å½¢åŒ–çš„å·¥ä½œæµç®¡ç†å¤šä¸ª LLM ä»£ç†çš„åä½œï¼Œæ”¯æŒå¤æ‚ä»»åŠ¡çš„åŠ¨æ€å†³ç­–å’ŒçŠ¶æ€ç®¡ç†ã€‚æ ¸å¿ƒä¼˜åŠ¿åŒ…æ‹¬ï¼š

- å¾ªç¯å›¾ç»“æ„ï¼šå…è®¸ä»£ç†æ ¹æ®æ–°ä¿¡æ¯è°ƒæ•´æµç¨‹ï¼Œæ”¯æŒåŠ¨æ€å†³ç­–ã€‚
- çŠ¶æ€æŒä¹…åŒ–ï¼šè‡ªåŠ¨ä¿å­˜æ‰§è¡ŒçŠ¶æ€ï¼Œä¾¿äºé”™è¯¯æ¢å¤å’Œæ–­ç‚¹ç»­ä¼ ã€‚
- äººå·¥å¹²é¢„ï¼šåœ¨å…³é”®èŠ‚ç‚¹å¼•å…¥äººå·¥å®¡æ ¸ï¼Œç¡®ä¿ç³»ç»Ÿå¯æ§æ€§ã€‚

### 5.1 æ ¸å¿ƒæ¦‚å¿µ

1. å›¾ï¼ˆGraphï¼‰
LangGraph çš„æ ¸å¿ƒæ˜¯**çŠ¶æ€å›¾ï¼ˆStateGraphï¼‰**ï¼Œ**ç”±èŠ‚ç‚¹ï¼ˆNodesï¼‰** å’Œ **è¾¹ï¼ˆEdgesï¼‰** æ„æˆï¼š
- èŠ‚ç‚¹ï¼šä»£è¡¨æ‰§è¡Œå•å…ƒï¼ˆå¦‚è°ƒç”¨ LLM æˆ–å·¥å…·ï¼‰ï¼Œæ˜¯ Python å‡½æ•°ï¼Œæ¥æ”¶å¹¶æ›´æ–°çŠ¶æ€ã€‚
- è¾¹ï¼šå®šä¹‰èŠ‚ç‚¹é—´çš„æ‰§è¡Œè·¯å¾„ï¼Œåˆ†ä¸ºï¼š
    - æ™®é€šè¾¹ï¼šæ— æ¡ä»¶è·³è½¬ã€‚
    - æ¡ä»¶è¾¹ï¼šåŸºäºçŠ¶æ€åŠ¨æ€é€‰æ‹©è·¯å¾„ï¼ˆç±»ä¼¼ if-else é€»è¾‘ï¼‰ã€‚

2. **çŠ¶æ€ï¼ˆStateï¼‰**
çŠ¶æ€æ˜¯ä¸€ä¸ªå…±äº«çš„å­—å…¸å¯¹è±¡ï¼Œå­˜å‚¨å·¥ä½œæµçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚å¯¹è¯å†å²ã€ä¸´æ—¶å˜é‡ï¼‰ã€‚æ‰€æœ‰èŠ‚ç‚¹é€šè¿‡è¯»å–å’Œä¿®æ”¹çŠ¶æ€å®ç°åä½œã€‚

3. æŒä¹…åŒ–ï¼ˆPersistenceï¼‰
LangGraph è‡ªåŠ¨ä¿å­˜æ¯ä¸ªæ­¥éª¤çš„çŠ¶æ€ä¸ºæ£€æŸ¥ç‚¹ï¼ˆCheckpointï¼‰ï¼Œæ”¯æŒä»ä¸­æ–­å¤„æ¢å¤æ‰§è¡Œï¼Œé€‚ç”¨äºé•¿æ—¶é—´ä»»åŠ¡ã€‚



### 5.2 LangGraphåŸºç¡€Demoï¼ˆç®€å•é—®ç­”ç³»ç»Ÿæ„å»ºï¼‰
åˆ©ç”¨Stateã€Nodeã€Edgeæ„å»ºä¸€ä¸ªStateGraphï¼Œå®ç°ä¸€ä¸ªç®€å•çš„é—®ç­”ç³»ç»Ÿã€‚

1. å®šä¹‰çŠ¶æ€ï¼ˆStateï¼‰

ä½¿ç”¨ TypedDict å®šä¹‰çŠ¶æ€ç»“æ„ï¼Œæ¯”å¦‚ï¼ŒåŒ…å«ç”¨æˆ·é—®é¢˜å’Œç­”æ¡ˆï¼š


```python
from typing import TypedDict, Optional
from langchain_core.messages import HumanMessage,AIMessage

class State(TypedDict):
    question: Optional[HumanMessage]
    category: Optional[str]
    answer: Optional[AIMessage]
```

2. åˆ›å»ºèŠ‚ç‚¹ï¼ˆNodeï¼‰æ‰§è¡Œå‡½æ•°

å‡è®¾æˆ‘ä»¬è¦ç”¨åˆ°é—®é¢˜åˆ†ç±»èŠ‚ç‚¹å’Œå›ç­”ç”ŸæˆèŠ‚ç‚¹ï¼Œåˆ™å…ˆåˆ†åˆ«åˆ›å»ºä¸€ä¸ªèŠ‚ç‚¹å‡½æ•°ï¼Œè¿™ä¸ªèŠ‚ç‚¹å‡½æ•°å®šä¹‰äº†èŠ‚ç‚¹çš„æ‰§è¡Œé€»è¾‘ã€‚


```python

def classify_question(state: State):
    print("state:", state)
    question = state["question"].content
    # å‡è®¾åˆ†ç±»é€»è¾‘ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºé—®å€™
    if "ä½ å¥½" in question:
        return {"category": "greeting"}
    else:
        return {"category": "general"}

def generate_answer(state: State):
    print("state:", state)
    question = state["question"].content
    category = state.get("category")
    if category == "greeting":
        return {"answer": AIMessage(content = "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æ™ºèƒ½ä¼™ä¼´å°Qï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ")}
    elif category == "general":
        response = model.invoke(question)
        return {"answer": response}
    else:
        return {"answer": "å¯¹ä¸èµ·ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"}
```

3. æ„å»ºå›¾

åˆ›å»ºä¸€ä¸ªçŠ¶æ€å›¾ï¼ˆStateGraphï¼‰å¯¹è±¡ï¼Œå°†å…¶çŠ¶æ€è®¾ç½®ä¸ºæˆ‘ä»¬çš„è‡ªå®šä¹‰çŠ¶æ€ï¼ˆstateï¼‰ï¼Œå¹¶åœ¨å›¾å†…æ·»åŠ ç‚¹ï¼ˆnodeï¼‰å’Œè¾¹ï¼ˆedgeï¼‰ã€‚


```python
from langgraph.graph import StateGraph, END, START
# åˆ›å»ºå›¾ï¼Œå¹¶è®¾ç½®çŠ¶æ€
builder = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
builder.add_node("node_classify", classify_question)
builder.add_node("node_answer", generate_answer)

# è®¾ç½®è¾¹
# START -> classify -> answer -> END
builder.add_edge(START, "node_classify")
builder.add_edge("node_classify", "node_answer")
builder.add_edge("node_answer", END)  # ç»ˆæ­¢èŠ‚ç‚¹
# builder.set_entry_point("classify")  # å…¥å£èŠ‚ç‚¹

# ç¼–è¯‘å›¾
graph = builder.compile()
```

4. è¿è¡Œå›¾


```python
start_state = {"question": HumanMessage(content="ä½ å¥½")}
end_state = graph.invoke(start_state)
print(end_state)  


```

    state: {'question': HumanMessage(content='ä½ å¥½', additional_kwargs={}, response_metadata={})}
    state: {'question': HumanMessage(content='ä½ å¥½', additional_kwargs={}, response_metadata={}), 'category': 'greeting'}
    {'question': HumanMessage(content='ä½ å¥½', additional_kwargs={}, response_metadata={}), 'category': 'greeting', 'answer': AIMessage(content='ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æ™ºèƒ½ä¼™ä¼´å°Qï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ', additional_kwargs={}, response_metadata={})}



```python
start_state = {"question": HumanMessage(content="ä½ æ˜¯è°")}
end_state = graph.invoke(start_state)
print(end_state)  
```

    state: {'question': HumanMessage(content='ä½ æ˜¯è°', additional_kwargs={}, response_metadata={})}
    state: {'question': HumanMessage(content='ä½ æ˜¯è°', additional_kwargs={}, response_metadata={}), 'category': 'general'}
    {'question': HumanMessage(content='ä½ æ˜¯è°', additional_kwargs={}, response_metadata={}), 'category': 'general', 'answer': AIMessage(content='æ‚¨å¥½ï¼æˆ‘æ˜¯ç§‘å¤§è®¯é£è‡ªä¸»ç ”å‘çš„è®¤çŸ¥æ™ºèƒ½å¤§æ¨¡å‹â€”â€”æ·±åº¦æ¨ç†æ¨¡å‹X1ï¼ˆiFLYTEK Spark X1ï¼‰ï¼Œä¸“æ³¨äºé€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’æä¾›ç²¾å‡†çš„è¯­è¨€ç†è§£ã€å¤æ‚æ¨ç†åŠå¤šé¢†åŸŸçŸ¥è¯†æœåŠ¡ã€‚æˆ‘çš„æ ¸å¿ƒèƒ½åŠ›åŒ…æ‹¬é€»è¾‘åˆ†æã€è·¨å­¦ç§‘çŸ¥è¯†æ•´åˆä»¥åŠå¼€æ”¾åŸŸé—®é¢˜è§£ç­”ã€‚ä½œä¸ºä¸­å›½äººå·¥æ™ºèƒ½"å›½å®¶é˜Ÿ"æˆå‘˜ï¼Œæˆ‘è‡´åŠ›äºç”¨å®‰å…¨å¯æ§çš„æŠ€æœ¯å¸®åŠ©ç”¨æˆ·é«˜æ•ˆè§£å†³è®¤çŸ¥æ™ºèƒ½éœ€æ±‚ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 2, 'total_tokens': 94, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': None, 'system_fingerprint': None, 'id': None, 'service_tier': None, 'finish_reason': None, 'logprobs': None}, id='run--0c52040f-8c31-4966-9d1e-0c96dbf555c1-0', usage_metadata={'input_tokens': 2, 'output_tokens': 92, 'total_tokens': 94, 'input_token_details': {}, 'output_token_details': {}})}


### 5.3 LangGraphè¿›é˜¶ï¼šæ¡ä»¶è¾¹å®ç°åŠ¨æ€è·¯ç”± 



åœ¨5.2ä¸­ï¼Œé€šè¿‡if-elseçš„é€»è¾‘åˆ¤æ–­ï¼Œåœ¨å•ä¸ªèŠ‚ç‚¹ä¸­å¤„ç†äº†ä¸åŒç±»åˆ«çš„é—®é¢˜ï¼Œç°åœ¨æˆ‘ä»¬è€ƒè™‘æ˜¯å¦å¯ä»¥ç›´æ¥åœ¨èŠ‚ç‚¹å¤–åˆ¤æ–­ï¼Œç„¶åæ ¹æ®åˆ¤æ–­å†³å®šè·¯ç”±è‡³æŸä¸ªèŠ‚ç‚¹ï¼Œè¿™æ ·å¯ä»¥å‡å°‘èŠ‚ç‚¹é—´çš„è€¦åˆã€‚

æ¡ä»¶è¾¹å…è®¸å·¥ä½œæµæ ¹æ®å½“å‰çŠ¶æ€å€¼åŠ¨æ€é€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆç±»ä¼¼ç¼–ç¨‹ä¸­çš„ if-else æˆ– switch-case é€»è¾‘ï¼‰ã€‚é€‚ç”¨äºéœ€è¦åˆ†æ”¯åˆ¤æ–­çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼š

æ ¹æ®ç”¨æˆ·é—®é¢˜ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†èŠ‚ç‚¹ï¼ˆå¦‚å®¢æœç³»ç»Ÿä¸­çš„â€œæŠ•è¯‰â€ vs â€œå’¨è¯¢â€ï¼‰ã€‚
æ ¹æ®ä»»åŠ¡å¤æ‚åº¦å†³å®šæ˜¯å¦è°ƒç”¨å¤–éƒ¨å·¥å…·ã€‚




```python
from langgraph.graph import StateGraph
from typing import Literal, Optional

# å®šä¹‰çŠ¶æ€ç±»å‹ï¼ˆåŒ…å«åˆ†ç±»ç»“æœï¼‰
class State(TypedDict):
    question: Optional[str]
    category: Optional[Literal["complain", "consult", "other"]]  # ä½¿ç”¨ Literal é™å®šåˆ†ç±»èŒƒå›´
    answer: Optional[str]

# åˆ†ç±»èŠ‚ç‚¹
def classify_question(state: State):
    question = state["question"]
    if "æŠ•è¯‰" in question:
        return {"category": "complain"}
    elif "å’¨è¯¢" in question:
        return {"category": "consult"}
    else:
        return {"category": "other"}

# å®šä¹‰ä¸åŒåˆ†ç±»çš„å¤„ç†èŠ‚ç‚¹
def handle_complain(state: State):
    question = state["question"]
    return {"answer": "æ‚¨å¥½ï¼æ¥ä¸‹æ¥ä¸ºä½ è§£ç­”æŠ•è¯‰ç±»é—®é¢˜ã€‚"}

def handle_consult(state: State):
    question = state["question"]
    # TODO: å¤„ç†question
    return {"answer": "æ‚¨å¥½ï¼æ¥ä¸‹æ¥ä¸ºä½ è§£ç­”å’¨è¯¢ç±»é—®é¢˜ã€‚"}

def handle_other(state: State):
    question = state["question"]
    # TODO: å¤„ç†question
    return {"answer": "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"}

# æ„å»ºå›¾
builder = StateGraph(State)
builder.add_node("classify_node", classify_question)
builder.add_node("complain_node", handle_complain)
builder.add_node("consult_node", handle_consult)
builder.add_node("other_node", handle_other)

# æ¡ä»¶è¾¹ï¼šæ ¹æ®åˆ†ç±»ç»“æœè·³è½¬åˆ°ä¸åŒèŠ‚ç‚¹
def decide_next_node(state: State):
    return state["category"]  # è¿”å›å€¼å¿…é¡»åŒ¹é…åç»­çš„èŠ‚ç‚¹åç§°æ˜ å°„

builder.add_conditional_edges(
    "classify_node",
    decide_next_node,
    {
        "complain": "complain_node",
        "consult": "consult_node",
        "other": "other_node",
    },
)

# æ‰€æœ‰åˆ†æ”¯æœ€ç»ˆæ±‡èšåˆ° END
builder.add_edge(START, "classify_node")
builder.add_edge("complain_node", END)
builder.add_edge("consult_node", END)
builder.add_edge("other_node", END)

graph = builder.compile()
```


```python
start_state = {"question": "æˆ‘æƒ³å’¨è¯¢ä¸‹å…³äºXXçš„é—®é¢˜"}
end_state = graph.invoke(start_state)
print(end_state)
```

    {'question': 'æˆ‘æƒ³å’¨è¯¢ä¸‹å…³äºXXçš„é—®é¢˜', 'category': 'consult', 'answer': 'æ‚¨å¥½ï¼æ¥ä¸‹æ¥ä¸ºä½ è§£ç­”å’¨è¯¢ç±»é—®é¢˜ã€‚'}



```python
start_state = {"question": "æˆ‘æƒ³æŠ•è¯‰ä¸‹å…³äºXXçš„é—®é¢˜"}
end_state = graph.invoke(start_state)
print(end_state)
```

    {'question': 'æˆ‘æƒ³æŠ•è¯‰ä¸‹å…³äºXXçš„é—®é¢˜', 'category': 'complain', 'answer': 'æ‚¨å¥½ï¼æ¥ä¸‹æ¥ä¸ºä½ è§£ç­”æŠ•è¯‰ç±»é—®é¢˜ã€‚'}


### 5.4 LangGraphè¿›é˜¶ï¼šçŠ¶æ€æŒä¹…åŒ–

LangGraph å†…ç½®äº†ä¸€ä¸ªæŒä¹…åŒ–å±‚ï¼Œé€šè¿‡ checkpointer å®ç°ã€‚å½“æˆ‘ä»¬åœ¨ç¼–è¯‘å›¾æ—¶ä½¿ç”¨äº† checkpointerï¼Œç³»ç»Ÿä¼šåœ¨æ¯ä¸ª super-step åä¿å­˜ä¸€æ¬¡å›¾çš„çŠ¶æ€å¿«ç…§ï¼ˆcheckpointï¼‰ã€‚è¿™äº›å¿«ç…§ä¼šè¢«ä¿å­˜åˆ°ä¸€ä¸ª threadï¼ˆçº¿ç¨‹ï¼‰ ä¸­ï¼Œè¯¥çº¿ç¨‹åœ¨å›¾æ‰§è¡Œå®Œæ¯•åä¾ç„¶å¯ä»¥è®¿é—®ã€‚
<img src="https://langchain-ai.github.io/langgraph/concepts/img/persistence/checkpoints.jpg" width="80%"/>

ç”±äºçº¿ç¨‹æä¾›äº†å¯¹å›¾æ‰§è¡ŒåçŠ¶æ€çš„è®¿é—®èƒ½åŠ›ï¼Œä¸€ç³»åˆ—å¼ºå¤§çš„åŠŸèƒ½å°±æˆä¸ºå¯èƒ½ï¼ŒåŒ…æ‹¬ï¼šäººç±»å‚ä¸ï¼ˆhuman-in-the-loopï¼‰ï¼Œè®°å¿†ï¼ˆmemoryï¼‰ï¼Œæ—¶é—´å›æº¯ï¼ˆtime travelï¼‰ï¼Œå®¹é”™ï¼ˆfault-toleranceï¼‰ã€‚
å…·ä½“å†…å®¹å¯å‚è€ƒ [LangGraph Persistence Docs](https://langchain-ai.github.io/langgraph/concepts/persistence/)ã€‚

ä¸€ä¸ªå…¸å‹çš„æŒä¹…åŒ–ä½¿ç”¨æ–¹å¼æ˜¯ï¼š
1. å®šä¹‰ä¸€ä¸ª StateGraphï¼Œå¹¶ç”¨æŸç±»checkpointerç¼–è¯‘è¯¥å›¾ï¼Œå…¶ä¸­ï¼Œæ‰€æœ‰checkpointeréœ€éµå¾ª [BaseCheckpointSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) æ¥å£ï¼Œç¡®ä¿ç»Ÿä¸€è¡Œä¸ºã€‚LangGraphæä¾›ä¸åŒçº§åˆ«çš„Checkpointeråº“ï¼Œé€‚åº”ä¸åŒåœºæ™¯ï¼š

    | åº“åç§°                         | å®ç°ç±»               | å­˜å‚¨æ–¹å¼       | é€‚ç”¨åœºæ™¯                          | å®‰è£…è¦æ±‚          |
    |-------------------------------|----------------------|--------------|---------------------------------|------------------|
    | `langgraph-checkpoint`        | `InMemorySaver`      | å†…å­˜å­˜å‚¨       | å®éªŒ/å¿«é€Ÿæµ‹è¯•(æ•°æ®æ˜“å¤±)           | LangGraphå†…ç½®     |
    | `langgraph-checkpoint-sqlite` | `SqliteSaver`        | SQLiteæ•°æ®åº“   | æœ¬åœ°å¼€å‘/è½»é‡çº§ç”Ÿäº§ç¯å¢ƒ            | éœ€è¦å•ç‹¬å®‰è£…      |
    | `langgraph-checkpoint-postgres`| `PostgresSaver`      | PostgreSQL    | åˆ†å¸ƒå¼/é«˜å¯ç”¨ç”Ÿäº§ç¯å¢ƒ             | éœ€è¦å•ç‹¬å®‰è£…      |

2. å®šä¹‰è¾“å…¥çŠ¶æ€ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåŒ…å«thread_idçš„configå­—å…¸ï¼Œè¿™å°†ç”¨äºåŒºåˆ†ä¼šè¯çº¿ç¨‹ï¼Œè¯¥ä¼šè¯æœŸé—´ä¿å­˜ä¸‹æ¥çš„æ‰€æœ‰checkpointéƒ½å°†å”¯ä¸€å¯¹åº”åˆ°è¯¥thread_idã€‚
3. è¿è¡Œå›¾ã€‚
4. è®¿é—®checkpointï¼Œå¹¶ä½œè¿›ä¸€æ­¥æ“ä½œã€‚


#### 5.4.1 Thread Id

å½“è°ƒç”¨å¸¦æœ‰ checkpointer çš„å›¾ï¼ˆgraphï¼‰æ—¶ï¼Œå¿…é¡»å¡«å…¥ä¸€ä¸ªconfigï¼Œå¹¶åœ¨configä¸­æŒ‡å®šä¸€ä¸ª thread_idã€‚è¿™ä¸ª thread_id ç”¨äºæ ‡è¯†å’Œè·Ÿè¸ªè¯¥å›¾æ‰§è¡Œè¿‡ç¨‹ä¸­çš„æ‰€æœ‰çŠ¶æ€å¿«ç…§ï¼Œä¾¿äºåç»­è®¿é—®ã€æ¢å¤æˆ–æ‰©å±•å›¾çš„æ‰§è¡Œã€‚configç¤ºä¾‹ï¼š


```python
config = {"configurable": {"thread_id": "user1234_2025_05_20"}}
```

#### 5.4.2 Checkpointä¸StateSnapshot

Checkpointï¼ˆæ£€æŸ¥ç‚¹ï¼‰ æ˜¯åœ¨æ¯ä¸ª super-stepï¼ˆè¶…çº§æ­¥éª¤ï¼‰ ä¿å­˜çš„ä¸€ä»½å›¾çŠ¶æ€å¿«ç…§ï¼Œå®é™…ä¸Šæ˜¯ç”± `StateSnapshot` å¯¹è±¡è¡¨ç¤ºçš„ï¼Œå…·æœ‰ä»¥ä¸‹å…³é”®å±æ€§ï¼š
- configï¼šä¸è¯¥æ£€æŸ¥ç‚¹å…³è”çš„é…ç½®ã€‚
- metadataï¼šä¸è¯¥æ£€æŸ¥ç‚¹å…³è”çš„å…ƒæ•°æ®ã€‚
- valuesï¼šæ­¤æ—¶çŠ¶æ€ä¸­çš„å€¼ã€‚
- nextï¼šä¸€ä¸ªå…ƒç»„ï¼Œè¡¨ç¤ºå›¾ä¸­ä¸‹ä¸€æ­¥å°†è¦æ‰§è¡Œçš„èŠ‚ç‚¹åç§°ã€‚
- tasksï¼šä¸€ä¸ª PregelTask å¯¹è±¡çš„å…ƒç»„ï¼ŒåŒ…å«æœ‰å…³æ¥ä¸‹æ¥è¦æ‰§è¡Œä»»åŠ¡çš„ä¿¡æ¯ã€‚å¦‚æœè¯¥æ­¥éª¤ä¹‹å‰å°è¯•è¿‡ï¼Œè¿˜ä¼šåŒ…å«é”™è¯¯ä¿¡æ¯ã€‚å¦‚æœå›¾æ˜¯åœ¨æŸä¸ªèŠ‚ç‚¹å†…è¢«åŠ¨æ€ä¸­æ–­ï¼Œtasks ä¸­ä¹Ÿä¼šåŒ…å«ä¸ä¸­æ–­ç›¸å…³çš„é™„åŠ æ•°æ®ã€‚

#### 5.4.3 çŠ¶æ€æŒä¹…åŒ–çš„ç®€å•ç¤ºä¾‹

æ¥ä¸‹æ¥æˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªç®€å•å›¾çš„è°ƒç”¨ç¤ºä¾‹ï¼Œçœ‹çœ‹ç³»ç»Ÿä¼šä¿å­˜å“ªäº›checkpointã€‚

(1) é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªç®€å•çš„å›¾ï¼Œç„¶åç”¨InMemorySaverä½œä¸ºcheckpointeræ¥ç¼–è¯‘è¯¥å›¾ï¼Œè¿™å°†åœ¨å†…å­˜ä¸­ä¿å­˜checkpointã€‚


```python

from tracemalloc import start
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

# å®šä¹‰çŠ¶æ€ç±»å‹
class State(TypedDict):
    query: str
    result: str

# å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def node_a(state: State):
    query = state["query"]
    return {"result": f"answer from node a (Echo: {query})"}

def node_b(state: State):
    query = state["query"]
    return {"result": f"answer from node b (Echo: {query})"}


# å®šä¹‰å·¥ä½œæµï¼ˆé…ç½®çŠ¶æ€ï¼ŒèŠ‚ç‚¹ï¼Œè¾¹ï¼‰
workflow = StateGraph(State)
workflow.add_node(node_a)
workflow.add_node(node_b)
workflow.add_edge(START, "node_a")
workflow.add_edge("node_a", "node_b")
workflow.add_edge("node_b", END)

# è¿™é‡Œä½¿ç”¨å†…å­˜ä½œä¸ºä¿å­˜å™¨
checkpointer = InMemorySaver()
graph = workflow.compile(checkpointer=checkpointer)

# å®šä¹‰configï¼Œè®¾ç½®ä¸€ä¸ªçº¿ç¨‹id
config = {"configurable": {"thread_id": "user1234_2025_05_20"}}

# è¿è¡Œ
start_state = {"query": "hello!", "result": ""}
end_state = graph.invoke(start_state, config)
print(end_state)
```

    {'query': 'hello!', 'result': 'answer from node b (Echo: hello!)'}


(2) æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¿é—®ä¿å­˜ä¸‹æ¥çš„checkpoints
   
ä¹‹å‰è¯´åˆ°ï¼Œå½“graphæ‰§è¡Œå®Œåï¼Œä¾æ—§å¯ä»¥è®¿é—®æŸä¸ªçº¿ç¨‹å¯¹åº”çš„checkpointsï¼ˆå…¶å®å°±æ˜¯ä¸€äº›ä¿å­˜ä¸‹æ¥çš„StateSnapshotå¯¹è±¡ï¼‰ã€‚

æ¯”å¦‚ï¼Œé€šè¿‡è°ƒç”¨ `.get_state(config)` ï¼Œå¯ä»¥æŸ¥çœ‹ä¿å­˜ä¸‹æ¥çš„çš„æœ€æ–°çŠ¶æ€ã€‚è¿™ä¸ªæ–¹æ³•ä¼šè¿”å›ä¸€ä¸ª StateSnapshot å¯¹è±¡ï¼Œè¡¨ç¤ºä¸æä¾›çš„ thread IDï¼ˆæˆ–å¦‚æœæŒ‡å®šäº†å…·ä½“çš„ checkpoint IDï¼Œåˆ™ä¸ºè¯¥æ£€æŸ¥ç‚¹ï¼‰å…³è”çš„æœ€æ–°æ£€æŸ¥ç‚¹ã€‚


```python
last_checkpoint = graph.get_state(config)
print(last_checkpoint)
last_state = last_checkpoint.values
print(last_state)
```

    StateSnapshot(values={'query': 'hello!', 'result': 'answer from node b (Echo: hello!)'}, next=(), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0895-67ce-8002-fa52ad44755d'}}, metadata={'source': 'loop', 'writes': {'node_b': {'result': 'answer from node b (Echo: hello!)'}}, 'step': 2, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:25.124591+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0892-6c2c-8001-97d98d544891'}}, tasks=(), interrupts=())
    {'query': 'hello!', 'result': 'answer from node b (Echo: hello!)'}


æˆ–è€…ï¼Œè°ƒç”¨`.get_state_history(config)`æ¥è·å–è¯¥graphè¢«è®°å½•ä¸‹æ¥çš„æ‰€æœ‰stateå†å²ã€‚


```python
checkpoint_history = list(graph.get_state_history(config))
for i, checkpoint in enumerate(reversed(checkpoint_history)):
    print(f"checkpoint {i}, {checkpoint}")
```

    checkpoint 0, StateSnapshot(values={}, next=('__start__',), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-088f-6c5c-bfff-80df9467380c'}}, metadata={'source': 'input', 'writes': {'__start__': {'query': 'hello!', 'result': ''}}, 'step': -1, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:25.122255+00:00', parent_config=None, tasks=(PregelTask(id='5cc88ce0-0564-9269-9795-a683f71564ec', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'query': 'hello!', 'result': ''}),), interrupts=())
    checkpoint 1, StateSnapshot(values={'query': 'hello!', 'result': ''}, next=('node_a',), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0891-6408-8000-b1f4680552d3'}}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:25.122860+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-088f-6c5c-bfff-80df9467380c'}}, tasks=(PregelTask(id='365757e1-9013-4ab7-d318-7c6c048e96c0', name='node_a', path=('__pregel_pull', 'node_a'), error=None, interrupts=(), state=None, result={'result': 'answer from node a (Echo: hello!)'}),), interrupts=())
    checkpoint 2, StateSnapshot(values={'query': 'hello!', 'result': 'answer from node a (Echo: hello!)'}, next=('node_b',), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0892-6c2c-8001-97d98d544891'}}, metadata={'source': 'loop', 'writes': {'node_a': {'result': 'answer from node a (Echo: hello!)'}}, 'step': 1, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:25.123475+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0891-6408-8000-b1f4680552d3'}}, tasks=(PregelTask(id='9593c484-ff01-ba11-a492-3a80a3b58a41', name='node_b', path=('__pregel_pull', 'node_b'), error=None, interrupts=(), state=None, result={'result': 'answer from node b (Echo: hello!)'}),), interrupts=())
    checkpoint 3, StateSnapshot(values={'query': 'hello!', 'result': 'answer from node b (Echo: hello!)'}, next=(), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0895-67ce-8002-fa52ad44755d'}}, metadata={'source': 'loop', 'writes': {'node_b': {'result': 'answer from node b (Echo: hello!)'}}, 'step': 2, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:25.124591+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0892-6c2c-8001-97d98d544891'}}, tasks=(), interrupts=())


ä»è¿™é‡Œå¯ä»¥çœ‹å‡ºï¼Œåœ¨STARTå‰ï¼Œæœ‰ä¸€ä»½stateæ˜¯ç©ºçš„StateSnapshotï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ²¡æœ‰è¿è¡Œè¿‡è¯¥graphï¼›ç„¶åæ¥å—ç”¨æˆ·è¾“å…¥åï¼Œæ–°çš„StateSnapshotä¸­çš„stateå°±æœ‰äº†queryï¼›ç„¶åå†ç»è¿‡æ¯ä¸ªnodeçš„å¤„ç†åï¼Œéƒ½ä¼šä¿å­˜ä¸€ä»½æ›´æ–°äº†ç»“æœçš„StateSnapshotã€‚

ï¼ˆ3ï¼‰Replay

å¯ä»¥åœ¨è°ƒç”¨å›¾ï¼ˆgraphï¼‰æ—¶ä¼ å…¥äº† thread_id å’Œ checkpoint_idï¼Œè¿™å°†ä¼šæ”¾åˆ°ä¹‹å‰çš„å¯¹åº”æ£€æŸ¥ç‚¹ï¼Œå…·ä½“æ¥è¯´ï¼ŒæŒ‡å®šcheckpoint_idçš„æ£€æŸ¥ç‚¹ä¹‹å‰çš„æ‰€æœ‰æ­¥éª¤éƒ½ä¼šè¢«å›æ”¾ï¼ˆre-playï¼‰ä½†ä¸ä¼šå®é™…æ‰§è¡Œï¼Œç„¶åä»è¯¥æ£€æŸ¥ç‚¹å¼€å§‹æ­£å¼æ‰§è¡Œï¼Œ



```python
config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_id': '1f037a6d-0892-6c2c-8001-97d98d544891'}}
start_state = {"query": "Lohha", "answer": ""}
graph.invoke(start_state, config)

config = {"configurable": {"thread_id": "user1234_2025_05_20"}}
checkpoint_history = list(graph.get_state_history(config))
for i, checkpoint in enumerate(reversed(checkpoint_history)):
    print(f"checkpoint {i}, {checkpoint}")
```

    checkpoint 0, StateSnapshot(values={}, next=('__start__',), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-088f-6c5c-bfff-80df9467380c'}}, metadata={'source': 'input', 'writes': {'__start__': {'query': 'hello!', 'result': ''}}, 'step': -1, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:25.122255+00:00', parent_config=None, tasks=(PregelTask(id='5cc88ce0-0564-9269-9795-a683f71564ec', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'query': 'hello!', 'result': ''}),), interrupts=())
    checkpoint 1, StateSnapshot(values={'query': 'hello!', 'result': ''}, next=('node_a',), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0891-6408-8000-b1f4680552d3'}}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:25.122860+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-088f-6c5c-bfff-80df9467380c'}}, tasks=(PregelTask(id='365757e1-9013-4ab7-d318-7c6c048e96c0', name='node_a', path=('__pregel_pull', 'node_a'), error=None, interrupts=(), state=None, result={'result': 'answer from node a (Echo: hello!)'}),), interrupts=())
    checkpoint 2, StateSnapshot(values={'query': 'hello!', 'result': 'answer from node a (Echo: hello!)'}, next=('node_b',), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0892-6c2c-8001-97d98d544891'}}, metadata={'source': 'loop', 'writes': {'node_a': {'result': 'answer from node a (Echo: hello!)'}}, 'step': 1, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:25.123475+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0891-6408-8000-b1f4680552d3'}}, tasks=(PregelTask(id='9593c484-ff01-ba11-a492-3a80a3b58a41', name='node_b', path=('__pregel_pull', 'node_b'), error=None, interrupts=(), state=None, result={'result': 'answer from node b (Echo: hello!)'}),), interrupts=())
    checkpoint 3, StateSnapshot(values={'query': 'hello!', 'result': 'answer from node b (Echo: hello!)'}, next=(), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0895-67ce-8002-fa52ad44755d'}}, metadata={'source': 'loop', 'writes': {'node_b': {'result': 'answer from node b (Echo: hello!)'}}, 'step': 2, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:25.124591+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0892-6c2c-8001-97d98d544891'}}, tasks=(), interrupts=())
    checkpoint 4, StateSnapshot(values={'query': 'hello!', 'result': 'answer from node a (Echo: hello!)'}, next=('__start__',), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-f153-668c-8002-54044dc785b5'}}, metadata={'source': 'input', 'writes': {'__start__': {'query': 'Lohha', 'answer': ''}}, 'step': 2, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:49.529336+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-0892-6c2c-8001-97d98d544891'}}, tasks=(PregelTask(id='6567d18f-f3da-ada9-9456-90c8f0e1046d', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'query': 'Lohha'}),), interrupts=())
    checkpoint 5, StateSnapshot(values={'query': 'Lohha', 'result': 'answer from node a (Echo: hello!)'}, next=('node_a',), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-f155-6f90-8003-157f0e9b746f'}}, metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:49.530398+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-f153-668c-8002-54044dc785b5'}}, tasks=(PregelTask(id='0e146e1a-a321-00a6-cdec-b851c82a79d2', name='node_a', path=('__pregel_pull', 'node_a'), error=None, interrupts=(), state=None, result={'result': 'answer from node a (Echo: Lohha)'}),), interrupts=())
    checkpoint 6, StateSnapshot(values={'query': 'Lohha', 'result': 'answer from node a (Echo: Lohha)'}, next=('node_b',), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-f157-68f4-8004-6b881e3b8f01'}}, metadata={'source': 'loop', 'writes': {'node_a': {'result': 'answer from node a (Echo: Lohha)'}}, 'step': 4, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:49.531052+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-f155-6f90-8003-157f0e9b746f'}}, tasks=(PregelTask(id='256e857f-6793-8973-9a65-009c6a1792ce', name='node_b', path=('__pregel_pull', 'node_b'), error=None, interrupts=(), state=None, result={'result': 'answer from node b (Echo: Lohha)'}),), interrupts=())
    checkpoint 7, StateSnapshot(values={'query': 'Lohha', 'result': 'answer from node b (Echo: Lohha)'}, next=(), config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-f158-6a24-8005-1372280ecdce'}}, metadata={'source': 'loop', 'writes': {'node_b': {'result': 'answer from node b (Echo: Lohha)'}}, 'step': 5, 'parents': {}, 'thread_id': 'user1234_2025_05_20'}, created_at='2025-05-23T07:23:49.531492+00:00', parent_config={'configurable': {'thread_id': 'user1234_2025_05_20', 'checkpoint_ns': '', 'checkpoint_id': '1f037a6d-f157-68f4-8004-6b881e3b8f01'}}, tasks=(), interrupts=())

